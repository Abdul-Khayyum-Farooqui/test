[
  {
    "objectID": "results/technical-indicators.html",
    "href": "results/technical-indicators.html",
    "title": "Results & Insights",
    "section": "",
    "text": "Code\nimport warnings\nimport logging\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\ndf = pd.read_csv('final_data.csv')\n\n# SANITY CHECK: Ensure the DataFrame is not empty\nassert not df.empty, \"Error: The loaded DataFrame is empty.\"\n\ndf = df.fillna(0)\ndf[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].shift(1)\n\ndf_base = df.filter(items=['Date', 'High', 'Low', 'Close', 'Volume', \"Open\"])\ndf_technical = df.drop(columns=['reddit_sentiment', 'guardian_sentiment', 'reddit_score', 'nyt_sentiment'])\n\n# SANITY CHECK: Ensure subsets contain the expected columns\nexpected_base_columns = {'Date', 'High', 'Low', 'Close', 'Volume', 'Open'}\nmissing_base_columns = expected_base_columns - set(df_base.columns)\nassert not missing_base_columns, f\"Error: Missing expected columns in df_base: {missing_base_columns}\"\n\n\ndef random_forest_regression(df, dataset_name):\n    # Step 1: Data Preprocessing\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n    \n    # SANITY CHECK: Ensure 'Date' column is successfully converted to datetime\n    assert pd.api.types.is_datetime64_any_dtype(df.index), \"Error: 'Date' column is not properly converted to datetime.\"\n\n    # Create target variable (next day's High)\n    df['Target'] = df['High'].shift(-1)  # Predict next day's High\n\n    # Feature engineering\n    features = df.drop(columns=['High', 'Target'])\n    target = df['Target']\n\n    # Handle missing values after shifting\n    df.dropna(subset=['Target'], inplace=True)\n\n    # Split into train and test (last 30 days + 1 day prediction)\n    split_date = df.index[-31]\n    X_train, X_test = features[:split_date], features[split_date:]\n    y_train, y_test = target[:split_date], target[split_date:]\n\n    # Ensure X_train and X_test are pandas DataFrames with feature names\n    X_train = pd.DataFrame(X_train, columns=features.columns)\n    X_test = pd.DataFrame(X_test, columns=features.columns)\n\n    # Step 2: Train Random Forest Model\n    model_rf = RandomForestRegressor(\n        n_estimators=200,\n        max_depth=15,\n        min_samples_split=10,\n        min_samples_leaf=5,\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model_rf.fit(X_train, y_train)\n\n    # Step 3: Make Predictions\n    y_pred = model_rf.predict(X_test)\n\n    # Step 4: Estimate Confidence Intervals using Prediction Variance\n    # Get predictions from each tree in the forest\n    predictions_per_tree = np.array([tree.predict(X_test) for tree in model_rf.estimators_])\n\n    # Calculate mean and percentiles across trees\n    mean_predictions = predictions_per_tree.mean(axis=0)\n    lower_bound = np.percentile(predictions_per_tree, 5, axis=0)  # 5th percentile for 90% CI\n    upper_bound = np.percentile(predictions_per_tree, 95, axis=0)  # 95th percentile for 90% CI\n    \n    # SANITY CHECK: Ensure predictions have the same length as test data\n    assert len(mean_predictions) == len(y_test), \"Error: Length mismatch between predicted and actual values.\"\n    assert len(lower_bound) == len(y_test), \"Error: Length mismatch between lower bound predictions and actual values.\"\n    assert len(upper_bound) == len(y_test), \"Error: Length mismatch between upper bound predictions and actual values.\"\n\n    # Step 5: Feature Importance\n    feature_importance = pd.DataFrame({\n        'Feature': X_train.columns,\n        'Importance': model_rf.feature_importances_\n    }).sort_values(by='Importance', ascending=False)\n    \n    # SANITY CHECK: Ensure feature importance DataFrame is not empty\n    assert not feature_importance.empty, \"Error: Feature importance DataFrame is empty.\"\n\n\n    # Step 6: Plot Results using Plotly\n    fig1 = go.Figure()\n\n    # Actual Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=y_test[:-1],\n        mode='lines+markers',\n        name='Actual Price',\n        marker=dict(color='blue')\n    ))\n\n    # Predicted Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=mean_predictions[:-1],\n        mode='lines+markers',\n        name='Predicted Price',\n        line=dict(dash='dash'),\n        marker=dict(color='orange')\n    ))\n\n    # Confidence Interval\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1].tolist() + y_test.index[:-1].tolist()[::-1],\n        y=np.concatenate([lower_bound[:-1], upper_bound[:-1][::-1]]),\n        fill='toself',\n        fillcolor='rgba(128, 128, 128, 0.2)',\n        line=dict(color='rgba(255,255,255,0)'),\n        name='90% CI'\n    ))\n\n    # Next Day Prediction\n    last_date = y_test.index[-1] + pd.Timedelta(days=1)\n    fig1.add_trace(go.Scatter(\n        x=[last_date],\n        y=[mean_predictions[-1]],\n        mode='markers',\n        name=f'Next Day Prediction ({mean_predictions[-1]:.2f})',\n        marker=dict(color='red', size=10)\n    ))\n\n    fig1.update_layout(\n        title=f'Tesla Stock High Price Prediction with Random Forest&lt;br&gt;{dataset_name}',\n        xaxis_title='Date',\n        yaxis_title='Price ($)',\n        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.01),\n        template='plotly_white'\n    )\n    fig1.show()\n\n    # Feature Importance Plot\n    top_10_features = feature_importance.head(10)\n    fig2 = go.Figure()\n\n    fig2.add_trace(go.Bar(\n        y=top_10_features['Feature'],\n        x=top_10_features['Importance'],\n        orientation='h',\n        marker=dict(color='skyblue', line=dict(color='black', width=1)),\n        name='Feature Importance'\n    ))\n\n    # ‚û°Ô∏è Updated Layout Configuration for Figure 2\n    fig2.update_layout(\n        title={\n            'text': f'Top 10 Most Important Features&lt;br&gt;{dataset_name}',\n            'y': 0.95,                 # Pushed higher to avoid overlap\n            'x': 0.5,                  # Centered horizontally\n            'xanchor': 'center',\n            'yanchor': 'top',\n            'font': {'size': 18}       # Slightly larger for clarity\n        },\n        xaxis_title='Feature Importance Score',\n        yaxis_title='Features',\n        yaxis=dict(autorange=\"reversed\"),\n        margin=dict(l=60, r=40, t=60, b=40),  # Extra space to avoid crowding\n        template='plotly_white',\n        height=500  # Slightly taller for better readability\n    )\n\n    fig2.show()\n\n    # Step 7: Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(y_test[:-1], mean_predictions[:-1]))\n\n    return {\n        'rmse': rmse,\n        'next_day_prediction': mean_predictions[-1]\n    }\n\n\n\n\n\n#SEGMENT: Analyzing Data with Base Features + Technical Indicators : RSI, MACD, etc\nprint(\"Analyzing Data with Base Features + Technical Indicators : RSI, MACD, etc\\n\")\ntechnical = random_forest_regression(df_technical, \"Base Data + Technical Indicators\")\nprint(technical)\n\n\nAnalyzing Data with Base Features + Technical Indicators : RSI, MACD, etc\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n{'rmse': np.float64(9.756950435689252), 'next_day_prediction': np.float64(278.78239886219393)}"
  },
  {
    "objectID": "results/technical-indicators.html#stock-price-prediction-analysis",
    "href": "results/technical-indicators.html#stock-price-prediction-analysis",
    "title": "Results & Insights",
    "section": "",
    "text": "In this section, we present the analysis of Tesla‚Äôs stock price prediction using Random Forest Regression. The study is divided into three main parts: 1. Base Features Only - Traditional stock metrics like Open, High, Low, Close, and Volume. 2. Base + Technical Indicators - Including indicators like RSI, MACD, and Bollinger Bands. 3. Base + Technical Indicators + Sentiment Analysis - Combining price metrics, technical analysis, and sentiment data.\n\n\n\nCode\nimport warnings\nimport logging\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\ndf = pd.read_csv('final_data.csv')\n\n# SANITY CHECK: Ensure the DataFrame is not empty\nassert not df.empty, \"Error: The loaded DataFrame is empty.\"\n\ndf = df.fillna(0)\ndf[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].shift(1)\n\ndf_base = df.filter(items=['Date', 'High', 'Low', 'Close', 'Volume', \"Open\"])\ndf_technical = df.drop(columns=['reddit_sentiment', 'guardian_sentiment', 'reddit_score', 'nyt_sentiment'])\n\n# SANITY CHECK: Ensure subsets contain the expected columns\nexpected_base_columns = {'Date', 'High', 'Low', 'Close', 'Volume', 'Open'}\nmissing_base_columns = expected_base_columns - set(df_base.columns)\nassert not missing_base_columns, f\"Error: Missing expected columns in df_base: {missing_base_columns}\"\n\n\ndef random_forest_regression(df, dataset_name):\n    # Step 1: Data Preprocessing\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n    \n    # SANITY CHECK: Ensure 'Date' column is successfully converted to datetime\n    assert pd.api.types.is_datetime64_any_dtype(df.index), \"Error: 'Date' column is not properly converted to datetime.\"\n\n    # Create target variable (next day's High)\n    df['Target'] = df['High'].shift(-1)  # Predict next day's High\n\n    # Feature engineering\n    features = df.drop(columns=['High', 'Target'])\n    target = df['Target']\n\n    # Handle missing values after shifting\n    df.dropna(subset=['Target'], inplace=True)\n\n    # Split into train and test (last 30 days + 1 day prediction)\n    split_date = df.index[-31]\n    X_train, X_test = features[:split_date], features[split_date:]\n    y_train, y_test = target[:split_date], target[split_date:]\n\n    # Ensure X_train and X_test are pandas DataFrames with feature names\n    X_train = pd.DataFrame(X_train, columns=features.columns)\n    X_test = pd.DataFrame(X_test, columns=features.columns)\n\n    # Step 2: Train Random Forest Model\n    model_rf = RandomForestRegressor(\n        n_estimators=200,\n        max_depth=15,\n        min_samples_split=10,\n        min_samples_leaf=5,\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model_rf.fit(X_train, y_train)\n\n    # Step 3: Make Predictions\n    y_pred = model_rf.predict(X_test)\n\n    # Step 4: Estimate Confidence Intervals using Prediction Variance\n    # Get predictions from each tree in the forest\n    predictions_per_tree = np.array([tree.predict(X_test) for tree in model_rf.estimators_])\n\n    # Calculate mean and percentiles across trees\n    mean_predictions = predictions_per_tree.mean(axis=0)\n    lower_bound = np.percentile(predictions_per_tree, 5, axis=0)  # 5th percentile for 90% CI\n    upper_bound = np.percentile(predictions_per_tree, 95, axis=0)  # 95th percentile for 90% CI\n    \n    # SANITY CHECK: Ensure predictions have the same length as test data\n    assert len(mean_predictions) == len(y_test), \"Error: Length mismatch between predicted and actual values.\"\n    assert len(lower_bound) == len(y_test), \"Error: Length mismatch between lower bound predictions and actual values.\"\n    assert len(upper_bound) == len(y_test), \"Error: Length mismatch between upper bound predictions and actual values.\"\n\n    # Step 5: Feature Importance\n    feature_importance = pd.DataFrame({\n        'Feature': X_train.columns,\n        'Importance': model_rf.feature_importances_\n    }).sort_values(by='Importance', ascending=False)\n    \n    # SANITY CHECK: Ensure feature importance DataFrame is not empty\n    assert not feature_importance.empty, \"Error: Feature importance DataFrame is empty.\"\n\n\n    # Step 6: Plot Results using Plotly\n    fig1 = go.Figure()\n\n    # Actual Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=y_test[:-1],\n        mode='lines+markers',\n        name='Actual Price',\n        marker=dict(color='blue')\n    ))\n\n    # Predicted Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=mean_predictions[:-1],\n        mode='lines+markers',\n        name='Predicted Price',\n        line=dict(dash='dash'),\n        marker=dict(color='orange')\n    ))\n\n    # Confidence Interval\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1].tolist() + y_test.index[:-1].tolist()[::-1],\n        y=np.concatenate([lower_bound[:-1], upper_bound[:-1][::-1]]),\n        fill='toself',\n        fillcolor='rgba(128, 128, 128, 0.2)',\n        line=dict(color='rgba(255,255,255,0)'),\n        name='90% CI'\n    ))\n\n    # Next Day Prediction\n    last_date = y_test.index[-1] + pd.Timedelta(days=1)\n    fig1.add_trace(go.Scatter(\n        x=[last_date],\n        y=[mean_predictions[-1]],\n        mode='markers',\n        name=f'Next Day Prediction ({mean_predictions[-1]:.2f})',\n        marker=dict(color='red', size=10)\n    ))\n\n    fig1.update_layout(\n        title=f'Tesla Stock High Price Prediction with Random Forest&lt;br&gt;{dataset_name}',\n        xaxis_title='Date',\n        yaxis_title='Price ($)',\n        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.01),\n        template='plotly_white'\n    )\n    fig1.show()\n\n    # Feature Importance Plot\n    top_10_features = feature_importance.head(10)\n    fig2 = go.Figure()\n\n    fig2.add_trace(go.Bar(\n        y=top_10_features['Feature'],\n        x=top_10_features['Importance'],\n        orientation='h',\n        marker=dict(color='skyblue', line=dict(color='black', width=1)),\n        name='Feature Importance'\n    ))\n\n    fig2.update_layout(\n        title=f'Top 10 Most Important Features&lt;br&gt;{dataset_name}',\n        xaxis_title='Feature Importance Score',\n        yaxis_title='Features',\n        yaxis=dict(autorange=\"reversed\"),\n        template='plotly_white'\n    )\n    fig2.show()\n\n    # Step 7: Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(y_test[:-1], mean_predictions[:-1]))\n\n    return {\n        'rmse': rmse,\n        'next_day_prediction': mean_predictions[-1]\n    }\n\n\n\n\n\n#SEGMENT: Analyzing Data with Base Features + Technical Indicators : RSI, MACD, etc\nprint(\"Analyzing Data with Base Features + Technical Indicators : RSI, MACD, etc\\n\")\ntechnical = random_forest_regression(df_technical, \"Base Data + Technical Indicators\")\nprint(technical)\n\n\nAnalyzing Data with Base Features + Technical Indicators : RSI, MACD, etc\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n{'rmse': np.float64(9.756950435689252), 'next_day_prediction': np.float64(278.78239886219393)}"
  },
  {
    "objectID": "results/technical-indicators.html#indicator",
    "href": "results/technical-indicators.html#indicator",
    "title": "Results & Insights",
    "section": "üîç Indicator",
    "text": "üîç Indicator\n\nRMSE: 9.76\nPlot\n\nThe model performs reasonably well in capturing the general trend of the stock prices but struggles slightly during periods of high volatility.\nThe confidence intervals provide valuable insights into the model‚Äôs certainty, allowing users to understand when predictions may be less reliable.\nThe close alignment between the actual and predicted prices suggests that the Random Forest model effectively leverages the base data (OHLCV) along with technical indicators like RSI, MACD, etc., to make accurate predictions.\n\nFeature Importance\n\nThe Close price remains the dominant factor influencing the model‚Äôs predictions, highlighting its critical role in determining future stock price movements.\nThe inclusion of technical indicators like TP and EMA_7 enhances the model‚Äôs predictive power by incorporating additional signals related to price dynamics and momentum.\nWhile technical indicators add value, their impact is secondary to the core OHLCV data, as evidenced by their relatively lower importance scores."
  },
  {
    "objectID": "results/technical-indicators.html#indicator-1",
    "href": "results/technical-indicators.html#indicator-1",
    "title": "Results & Insights",
    "section": "üîç Indicator",
    "text": "üîç Indicator\n\nRMSE: 7.83\nPlot\n\nThe model performs reasonably well in capturing the general trend of the stock prices but struggles slightly during periods of high volatility.\nThe confidence intervals provide valuable insights into the model‚Äôs certainty, allowing users to understand when predictions may be less reliable.\nThe close alignment between the actual and predicted prices suggests that the XGBoost model effectively leverages the base data (Open, High, Low, Close, Volume) along with technical indicators like RSI, MACD, etc., to make accurate predictions.\n\nFeature Importance\n\nThe Close price remains the dominant factor influencing the model‚Äôs predictions, highlighting its critical role in determining future stock price movements.\nThe inclusion of technical indicators like TP and EMA_7 enhances the model‚Äôs predictive power by incorporating additional signals related to price dynamics and momentum.\nWhile technical indicators add value, their impact is secondary to the core OHLCV data, as evidenced by their relatively lower importance scores."
  },
  {
    "objectID": "results/base-features.html",
    "href": "results/base-features.html",
    "title": "Results & Insights",
    "section": "",
    "text": "Code\nimport warnings\nimport logging\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\ndf = pd.read_csv('final_data.csv')\n\n# SANITY CHECK: Ensure the DataFrame is not empty\nassert not df.empty, \"Error: The loaded DataFrame is empty.\"\n\ndf = df.fillna(0)\ndf[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].shift(1)\n\ndf_base = df.filter(items=['Date', 'High', 'Low', 'Close', 'Volume', \"Open\"])\ndf_technical = df.drop(columns=['reddit_sentiment', 'guardian_sentiment', 'reddit_score', 'nyt_sentiment'])\n\n# SANITY CHECK: Ensure subsets contain the expected columns\nexpected_base_columns = {'Date', 'High', 'Low', 'Close', 'Volume', 'Open'}\nmissing_base_columns = expected_base_columns - set(df_base.columns)\nassert not missing_base_columns, f\"Error: Missing expected columns in df_base: {missing_base_columns}\"\n\n\ndef random_forest_regression(df, dataset_name):\n    # Step 1: Data Preprocessing\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n    \n    # SANITY CHECK: Ensure 'Date' column is successfully converted to datetime\n    assert pd.api.types.is_datetime64_any_dtype(df.index), \"Error: 'Date' column is not properly converted to datetime.\"\n\n    # Create target variable (next day's High)\n    df['Target'] = df['High'].shift(-1)  # Predict next day's High\n\n    # Feature engineering\n    features = df.drop(columns=['High', 'Target'])\n    target = df['Target']\n\n    # Handle missing values after shifting\n    df.dropna(subset=['Target'], inplace=True)\n\n    # Split into train and test (last 30 days + 1 day prediction)\n    split_date = df.index[-31]\n    X_train, X_test = features[:split_date], features[split_date:]\n    y_train, y_test = target[:split_date], target[split_date:]\n\n    # Ensure X_train and X_test are pandas DataFrames with feature names\n    X_train = pd.DataFrame(X_train, columns=features.columns)\n    X_test = pd.DataFrame(X_test, columns=features.columns)\n\n    # Step 2: Train Random Forest Model\n    model_rf = RandomForestRegressor(\n        n_estimators=200,\n        max_depth=15,\n        min_samples_split=10,\n        min_samples_leaf=5,\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model_rf.fit(X_train, y_train)\n\n    # Step 3: Make Predictions\n    y_pred = model_rf.predict(X_test)\n\n    # Step 4: Estimate Confidence Intervals using Prediction Variance\n    # Get predictions from each tree in the forest\n    predictions_per_tree = np.array([tree.predict(X_test) for tree in model_rf.estimators_])\n\n    # Calculate mean and percentiles across trees\n    mean_predictions = predictions_per_tree.mean(axis=0)\n    lower_bound = np.percentile(predictions_per_tree, 5, axis=0)  # 5th percentile for 90% CI\n    upper_bound = np.percentile(predictions_per_tree, 95, axis=0)  # 95th percentile for 90% CI\n    \n    # SANITY CHECK: Ensure predictions have the same length as test data\n    assert len(mean_predictions) == len(y_test), \"Error: Length mismatch between predicted and actual values.\"\n    assert len(lower_bound) == len(y_test), \"Error: Length mismatch between lower bound predictions and actual values.\"\n    assert len(upper_bound) == len(y_test), \"Error: Length mismatch between upper bound predictions and actual values.\"\n\n    # Step 5: Feature Importance\n    feature_importance = pd.DataFrame({\n        'Feature': X_train.columns,\n        'Importance': model_rf.feature_importances_\n    }).sort_values(by='Importance', ascending=False)\n    \n    # SANITY CHECK: Ensure feature importance DataFrame is not empty\n    assert not feature_importance.empty, \"Error: Feature importance DataFrame is empty.\"\n\n\n    # Step 6: Plot Results using Plotly\n    fig1 = go.Figure()\n\n    # Actual Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=y_test[:-1],\n        mode='lines+markers',\n        name='Actual Price',\n        marker=dict(color='blue')\n    ))\n\n    # Predicted Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=mean_predictions[:-1],\n        mode='lines+markers',\n        name='Predicted Price',\n        line=dict(dash='dash'),\n        marker=dict(color='orange')\n    ))\n\n    # Confidence Interval\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1].tolist() + y_test.index[:-1].tolist()[::-1],\n        y=np.concatenate([lower_bound[:-1], upper_bound[:-1][::-1]]),\n        fill='toself',\n        fillcolor='rgba(128, 128, 128, 0.2)',\n        line=dict(color='rgba(255,255,255,0)'),\n        name='90% CI'\n    ))\n\n    # Next Day Prediction\n    last_date = y_test.index[-1] + pd.Timedelta(days=1)\n    fig1.add_trace(go.Scatter(\n        x=[last_date],\n        y=[mean_predictions[-1]],\n        mode='markers',\n        name=f'Next Day Prediction ({mean_predictions[-1]:.2f})',\n        marker=dict(color='red', size=10)\n    ))\n\n    fig1.update_layout(\n        title=f'Tesla Stock High Price Prediction with Random Forest&lt;br&gt;{dataset_name}',\n        xaxis_title='Date',\n        yaxis_title='Price ($)',\n        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.01),\n        template='plotly_white'\n    )\n    fig1.show()\n\n    # Feature Importance Plot\n    top_10_features = feature_importance.head(10).sort_values(by='Importance', ascending=True)\n\n    # Optional: Create a gradient color scale (you can remove if you want plain color)\n    colors = ['#1f77b4', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', '#ff7f0e']\n\n    fig2 = go.Figure()\n\n    fig2.add_trace(go.Bar(\n        y=top_10_features['Feature'],\n        x=top_10_features['Importance'],\n        orientation='h',\n        marker=dict(\n            color=\"lightblue\",\n            line=dict(color='black', width=1)\n        ),\n        name='Feature Importance',\n        hovertemplate='&lt;b&gt;%{y}&lt;/b&gt;: %{x:.4f}'  # Displays the score on hover\n    ))\n\n    # ‚û°Ô∏è Updated Layout Configuration for Figure 2\n    fig2.update_layout(\n        title={\n            'text': f'Top 10 Most Important Features&lt;br&gt;{dataset_name}',\n            'y': 0.95,                 # Pushed higher to avoid overlap\n            'x': 0.5,                  # Centered horizontally\n            'xanchor': 'center',\n            'yanchor': 'top',\n            'font': {'size': 18}       # Slightly larger for clarity\n        },\n        xaxis_title='Feature Importance Score',\n        yaxis_title='Features',\n        yaxis=dict(\n            autorange=False,\n            tickfont=dict(size=12),\n            categoryorder='total ascending'  # Ensures the most important is on top\n        ),\n        margin=dict(l=100, r=40, t=60, b=40),  # Extra space for better readability\n        template='plotly_white',\n        height=500  # Slightly taller for better clarity\n    )\n\n    fig2.show()\n\n    # Step 7: Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(y_test[:-1], mean_predictions[:-1]))\n\n    return {\n        'rmse': rmse,\n        'next_day_prediction': mean_predictions[-1]\n    }\n\n\n\n#SEGMENT: Analyzing Data with Only the Base Features : Open, High, Low, Close, Volume\nprint(\"Analyzing Data with Only the Base Features : Open, High, Low, Close, Volume\\n\")\nbase = random_forest_regression(df_base, \"Base Data\")\nprint(base)\n\n\nAnalyzing Data with Only the Base Features : Open, High, Low, Close, Volume\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n{'rmse': np.float64(13.538690981605868), 'next_day_prediction': np.float64(281.5759534159752)}"
  },
  {
    "objectID": "results/base-features.html#stock-price-prediction-analysis",
    "href": "results/base-features.html#stock-price-prediction-analysis",
    "title": "Results & Insights",
    "section": "",
    "text": "In this section, we present the analysis of Tesla‚Äôs stock price prediction using Random Forest Regression. The study is divided into three main parts: 1. Base Features Only - Traditional stock metrics like Open, High, Low, Close, and Volume. 2. Base + Technical Indicators - Including indicators like RSI, MACD, and Bollinger Bands. 3. Base + Technical Indicators + Sentiment Analysis - Combining price metrics, technical analysis, and sentiment data.\n\n\n\nCode\nimport warnings\nimport logging\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\ndf = pd.read_csv('final_data.csv')\n\n# SANITY CHECK: Ensure the DataFrame is not empty\nassert not df.empty, \"Error: The loaded DataFrame is empty.\"\n\ndf = df.fillna(0)\ndf[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].shift(1)\n\ndf_base = df.filter(items=['Date', 'High', 'Low', 'Close', 'Volume', \"Open\"])\ndf_technical = df.drop(columns=['reddit_sentiment', 'guardian_sentiment', 'reddit_score', 'nyt_sentiment'])\n\n# SANITY CHECK: Ensure subsets contain the expected columns\nexpected_base_columns = {'Date', 'High', 'Low', 'Close', 'Volume', 'Open'}\nmissing_base_columns = expected_base_columns - set(df_base.columns)\nassert not missing_base_columns, f\"Error: Missing expected columns in df_base: {missing_base_columns}\"\n\n\ndef random_forest_regression(df, dataset_name):\n    # Step 1: Data Preprocessing\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n    \n    # SANITY CHECK: Ensure 'Date' column is successfully converted to datetime\n    assert pd.api.types.is_datetime64_any_dtype(df.index), \"Error: 'Date' column is not properly converted to datetime.\"\n\n    # Create target variable (next day's High)\n    df['Target'] = df['High'].shift(-1)  # Predict next day's High\n\n    # Feature engineering\n    features = df.drop(columns=['High', 'Target'])\n    target = df['Target']\n\n    # Handle missing values after shifting\n    df.dropna(subset=['Target'], inplace=True)\n\n    # Split into train and test (last 30 days + 1 day prediction)\n    split_date = df.index[-31]\n    X_train, X_test = features[:split_date], features[split_date:]\n    y_train, y_test = target[:split_date], target[split_date:]\n\n    # Ensure X_train and X_test are pandas DataFrames with feature names\n    X_train = pd.DataFrame(X_train, columns=features.columns)\n    X_test = pd.DataFrame(X_test, columns=features.columns)\n\n    # Step 2: Train Random Forest Model\n    model_rf = RandomForestRegressor(\n        n_estimators=200,\n        max_depth=15,\n        min_samples_split=10,\n        min_samples_leaf=5,\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model_rf.fit(X_train, y_train)\n\n    # Step 3: Make Predictions\n    y_pred = model_rf.predict(X_test)\n\n    # Step 4: Estimate Confidence Intervals using Prediction Variance\n    # Get predictions from each tree in the forest\n    predictions_per_tree = np.array([tree.predict(X_test) for tree in model_rf.estimators_])\n\n    # Calculate mean and percentiles across trees\n    mean_predictions = predictions_per_tree.mean(axis=0)\n    lower_bound = np.percentile(predictions_per_tree, 5, axis=0)  # 5th percentile for 90% CI\n    upper_bound = np.percentile(predictions_per_tree, 95, axis=0)  # 95th percentile for 90% CI\n    \n    # SANITY CHECK: Ensure predictions have the same length as test data\n    assert len(mean_predictions) == len(y_test), \"Error: Length mismatch between predicted and actual values.\"\n    assert len(lower_bound) == len(y_test), \"Error: Length mismatch between lower bound predictions and actual values.\"\n    assert len(upper_bound) == len(y_test), \"Error: Length mismatch between upper bound predictions and actual values.\"\n\n    # Step 5: Feature Importance\n    feature_importance = pd.DataFrame({\n        'Feature': X_train.columns,\n        'Importance': model_rf.feature_importances_\n    }).sort_values(by='Importance', ascending=False)\n    \n    # SANITY CHECK: Ensure feature importance DataFrame is not empty\n    assert not feature_importance.empty, \"Error: Feature importance DataFrame is empty.\"\n\n\n    # Step 6: Plot Results using Plotly\n    fig1 = go.Figure()\n\n    # Actual Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=y_test[:-1],\n        mode='lines+markers',\n        name='Actual Price',\n        marker=dict(color='blue')\n    ))\n\n    # Predicted Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=mean_predictions[:-1],\n        mode='lines+markers',\n        name='Predicted Price',\n        line=dict(dash='dash'),\n        marker=dict(color='orange')\n    ))\n\n    # Confidence Interval\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1].tolist() + y_test.index[:-1].tolist()[::-1],\n        y=np.concatenate([lower_bound[:-1], upper_bound[:-1][::-1]]),\n        fill='toself',\n        fillcolor='rgba(128, 128, 128, 0.2)',\n        line=dict(color='rgba(255,255,255,0)'),\n        name='90% CI'\n    ))\n\n    # Next Day Prediction\n    last_date = y_test.index[-1] + pd.Timedelta(days=1)\n    fig1.add_trace(go.Scatter(\n        x=[last_date],\n        y=[mean_predictions[-1]],\n        mode='markers',\n        name=f'Next Day Prediction ({mean_predictions[-1]:.2f})',\n        marker=dict(color='red', size=10)\n    ))\n\n    fig1.update_layout(\n        title=f'Tesla Stock High Price Prediction with Random Forest&lt;br&gt;{dataset_name}',\n        xaxis_title='Date',\n        yaxis_title='Price ($)',\n        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.01),\n        template='plotly_white'\n    )\n    fig1.show()\n\n    # Feature Importance Plot\n    top_10_features = feature_importance.head(10)\n    fig2 = go.Figure()\n\n    fig2.add_trace(go.Bar(\n        y=top_10_features['Feature'],\n        x=top_10_features['Importance'],\n        orientation='h',\n        marker=dict(color='skyblue', line=dict(color='black', width=1)),\n        name='Feature Importance'\n    ))\n\n    fig2.update_layout(\n        title=f'Top 10 Most Important Features&lt;br&gt;{dataset_name}',\n        xaxis_title='Feature Importance Score',\n        yaxis_title='Features',\n        yaxis=dict(autorange=\"reversed\"),\n        template='plotly_white'\n    )\n    fig2.show()\n\n    # Step 7: Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(y_test[:-1], mean_predictions[:-1]))\n\n    return {\n        'rmse': rmse,\n        'next_day_prediction': mean_predictions[-1]\n    }\n\n\n\n#SEGMENT: Analyzing Data with Only the Base Features : Open, High, Low, Close, Volume\nprint(\"Analyzing Data with Only the Base Features : Open, High, Low, Close, Volume\\n\")\nbase = random_forest_regression(df_base, \"Base Data\")\nprint(base)\n\n\nAnalyzing Data with Only the Base Features : Open, High, Low, Close, Volume\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n{'rmse': np.float64(13.538690981605868), 'next_day_prediction': np.float64(281.5759534159752)}"
  },
  {
    "objectID": "results/base-features.html#rmse-13.54-points",
    "href": "results/base-features.html#rmse-13.54-points",
    "title": "Results & Insights",
    "section": "1Ô∏è‚É£ RMSE = 13.54 points",
    "text": "1Ô∏è‚É£ RMSE = 13.54 points"
  },
  {
    "objectID": "results/base-features.html#plot",
    "href": "results/base-features.html#plot",
    "title": "Results & Insights",
    "section": "2Ô∏è‚É£ Plot",
    "text": "2Ô∏è‚É£ Plot\n\nThe model performs reasonably well in capturing the general trend of the stock prices, but struggles slightly during periods of high volatility.\n\nThe confidence intervals provide valuable insights into the model‚Äôs certainty, allowing users to understand when predictions may be less reliable.\n\nThe close alignment between the actual and predicted prices suggests that the Random Forest model effectively leverages the base data (Open, High, Low, Close, Volume) to make accurate predictions."
  },
  {
    "objectID": "results/base-features.html#feature-importance",
    "href": "results/base-features.html#feature-importance",
    "title": "Results & Insights",
    "section": "3Ô∏è‚É£ Feature Importance",
    "text": "3Ô∏è‚É£ Feature Importance\n\nThe Close price is the dominant factor influencing the model‚Äôs predictions, highlighting its critical role in determining future stock price movements.\n\nThe Low and Open prices also contribute significantly, reinforcing the idea that intraday price movements (e.g., opening and low points) are important predictors.\n\nThe relatively low importance of Volume suggests that trading volume might not be as influential as price-related features in this specific context."
  },
  {
    "objectID": "results/base-features.html#base",
    "href": "results/base-features.html#base",
    "title": "Results & Insights",
    "section": "üîç Base",
    "text": "üîç Base\n\nRMSE: 12.97\nPlot\n\nThe model performs reasonably well in capturing the general trend of the stock prices but struggles slightly during periods of high volatility.\nThe confidence intervals provide valuable insights into the model‚Äôs certainty, allowing users to understand when predictions may be less reliable.\nThe close alignment between the actual and predicted prices suggests that the XGBoost model effectively leverages the base data (Open, High, Low, Close, Volume) to make accurate predictions.\n\nFeature Importance\n\nThe Close price is the dominant factor influencing the model‚Äôs predictions, highlighting its critical role in determining future stock price movements.\nThe Low and Open prices also contribute significantly, reinforcing the idea that intraday price movements (e.g., opening and low points) are important predictors.\nThe relatively low importance of Volume suggests that trading volume might not be as influential as price-related features in this specific context."
  },
  {
    "objectID": "exploratory-analysis.html",
    "href": "exploratory-analysis.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "We perform a correlation matrix analysis to understand the relationships between the various features in our dataset."
  },
  {
    "objectID": "exploratory-analysis.html#correlation-matrix-analysis",
    "href": "exploratory-analysis.html#correlation-matrix-analysis",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "We perform a correlation matrix analysis to understand the relationships between the various features in our dataset."
  },
  {
    "objectID": "exploratory-analysis.html#correlation-matrix-analysis-1",
    "href": "exploratory-analysis.html#correlation-matrix-analysis-1",
    "title": "Exploratory Analysis",
    "section": "Correlation Matrix Analysis",
    "text": "Correlation Matrix Analysis\nWe perform a correlation matrix analysis to understand the relationships between the various features in our dataset.\n\nInteractive Correlation Matrix\nBelow is the interactive correlation matrix heatmap generated using Plotly:\n\n\nCode\nimport matplotlib\nimport logging\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\nimport pandas as pd\nimport mplfinance as mpf\nimport matplotlib.pyplot as plt\n\n# Load and prepare data\ndf = pd.read_csv(\"final_data.csv\")\ndf['Date'] = pd.to_datetime(df['Date'])\ndf = df.tail(30)\ndf_candle = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']].copy()\ndf_candle.set_index('Date', inplace=True)\n\n# Custom style\ncustom_style = mpf.make_mpf_style(\n    base_mpf_style='classic',\n    rc={\n        'font.family': 'Times New Roman',\n        'axes.labelsize': 13,\n        'xtick.labelsize': 11,\n        'ytick.labelsize': 11\n    },\n    marketcolors=mpf.make_marketcolors(\n        up='#2ca02c',\n        down='#d62728',\n        edge='inherit',\n        wick='gray',\n        volume='lightblue'\n    ),\n    gridcolor='lightgray',\n    gridstyle='--',\n    facecolor='white',\n    figcolor='white'\n)\n\n# Overlays\napds = [\n    mpf.make_addplot(df['Bollinger_Upper_Band'], color='darkgreen', linestyle='--', width=1.2),\n    mpf.make_addplot(df['Bollinger_Lower_Band'], color='darkred', linestyle='--', width=1.2),\n    mpf.make_addplot(df['ATR'], panel=1, color='purple', width=1.3, ylabel='ATR'),\n    mpf.make_addplot(df['reddit_sentiment'], panel=2, color='navy', width=1.3, ylabel='Reddit Sentiment')\n]\n\n# Plot with figure/axes\nfig, axes = mpf.plot(\n    df_candle,\n    type='candle',\n    style=custom_style,\n    volume=True,\n    addplot=apds,\n    ylabel='Price ($)',\n    ylabel_lower='Volume',\n    panel_ratios=(3, 1, 1),\n    figratio=(16, 9),\n    figscale=1.5,\n    returnfig=True,\n    datetime_format='%b %d'\n)\n\n# Centered title on top panel\naxes[0].text(\n    0.5, 0.95,\n    'Tesla Volatility with Bollinger Bands, ATR & Reddit Sentiment',\n    transform=axes[0].transAxes,\n    fontsize=14,\n    fontweight='bold',\n    fontfamily='Times New Roman',\n    verticalalignment='top',\n    horizontalalignment='center'\n)\n\nplt.show()"
  },
  {
    "objectID": "exploratory-analysis.html#correlation-matrix-analysis-2",
    "href": "exploratory-analysis.html#correlation-matrix-analysis-2",
    "title": "Exploratory Analysis",
    "section": "Correlation Matrix Analysis",
    "text": "Correlation Matrix Analysis\nWe perform a correlation matrix analysis to understand the relationships between the various features in our dataset.\n\nInteractive Correlation Matrix\nBelow is the interactive correlation matrix heatmap generated using Plotly:\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom matplotlib.patches import Rectangle\n\n# Load dataset\ndf = pd.read_csv(\"final_data.csv\")\ndf.columns = df.columns.str.strip()\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Filter for March 24‚Äì27, 2025\nstart_date = pd.to_datetime(\"2025-03-24\")\nend_date = start_date + pd.Timedelta(days=3)\nmarch_df = df[(df['Date'] &gt;= start_date) & (df['Date'] &lt;= end_date)].copy()\nmarch_df['Daily_Return_%'] = march_df['Close'].pct_change() * 100\n\n# Create 3 subplots\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10), sharex=True, gridspec_kw={'height_ratios': [3, 1, 1]})\n\n# --- Candlestick Plot ---\nfor _, row in march_df.iterrows():\n    date_num = mdates.date2num(row['Date'])\n    color = 'green' if row['Close'] &gt;= row['Open'] else 'red'\n\n    # Wick\n    ax1.plot([row['Date'], row['Date']], [row['Low'], row['High']], color='black')\n\n    # Body\n    rect = Rectangle((date_num - 0.2, min(row['Open'], row['Close'])),\n                     0.4,\n                     abs(row['Close'] - row['Open']),\n                     color=color)\n    ax1.add_patch(rect)\n\nax1.set_ylabel(\"Price ($)\")\nax1.set_title(\"Tesla: Return %, and NYT Sentiment (March 24‚Äì27, 2025)\")\nax1.grid(True)\n\n# --- Daily Return Plot ---\nax2.bar(march_df['Date'], march_df['Daily_Return_%'], color='skyblue')\nax2.axhline(0, color='black', linestyle='--')\nax2.set_ylabel(\"Return (%)\")\nax2.grid(True)\n\n# --- FinBERT Sentiment Plot ---\nax3.plot(march_df['Date'], march_df['nyt_sentiment'], marker='o', linestyle='--', color='red')\nax3.set_ylabel(\"NEW YORK TIME Sentiment\")\nax3.set_xlabel(\"Date\")\nax3.grid(True)\n\n# Format x-axis\nax3.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\nfig.autofmt_xdate()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "exploratory-analysis.html#correlation-matrix-analysis-3",
    "href": "exploratory-analysis.html#correlation-matrix-analysis-3",
    "title": "Exploratory Analysis",
    "section": "Correlation Matrix Analysis",
    "text": "Correlation Matrix Analysis\nWe perform a correlation matrix analysis to understand the relationships between the various features in our dataset.\n\nInteractive Correlation Matrix\nBelow is the interactive correlation matrix heatmap generated using Plotly:\n\n\nCode\nimport matplotlib\nimport logging\nimport pandas as pd\nimport plotly.graph_objects as go\nimport numpy as np\n\n# üîá Suppress font and pandas warnings\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\npd.options.mode.chained_assignment = None  # &lt;--- This suppresses the SettingWithCopyWarning\n\n# Load the dataset\ndata = pd.read_csv('final_data.csv', parse_dates=['Date'])\ndata.sort_values('Date', inplace=True)\n\n# Filter the data for only the last 1 year\none_year_data = data[data['Date'] &gt;= (data['Date'].max() - pd.DateOffset(years=1))].copy()\n\n# Apply rolling average for smoothing (7-day window) using .loc to avoid warnings\none_year_data.loc[:, 'nyt_sentiment_smooth'] = one_year_data['nyt_sentiment'].rolling(window=7).mean()\none_year_data.loc[:, 'macd_smooth'] = one_year_data['MACD'].rolling(window=7).mean()\n\n# Compute correlation\ncorrelation = one_year_data[['nyt_sentiment_smooth', 'macd_smooth']].corr().iloc[0, 1]\n\n# --- Visualization ---\nfig = go.Figure()\n\n# Plot smoothed NYT sentiment\nfig.add_trace(go.Scatter(\n    x=one_year_data['Date'], \n    y=one_year_data['nyt_sentiment_smooth'], \n    mode='lines', \n    name='NYT Sentiment (Smoothed)', \n    line=dict(color='blue')\n))\n\n# Plot smoothed MACD\nfig.add_trace(go.Scatter(\n    x=one_year_data['Date'], \n    y=one_year_data['macd_smooth'], \n    mode='lines', \n    name='MACD (Smoothed)', \n    line=dict(color='green'),\n    yaxis='y2'\n))\n\n# Update layout for dual y-axis\nfig.update_layout(\n    title=f\"NYT Sentiment vs. MACD Over the Last Year (Correlation: {correlation:.2f})\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"NYT Sentiment\",\n    yaxis2=dict(title=\"MACD\", overlaying='y', side='right'),\n    legend_title=\"Metrics\",\n    template=\"plotly_white\"\n)\n\n# Display the plot\nfig.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About This Project",
    "section": "",
    "text": "This project dives deep into the relationship between stock price movements, market volatility, and public sentiment, with a dedicated focus on Tesla Inc.¬†Our goal is to understand how different market indicators, news events, Reddit discussions, and Twitter activity influence stock prices. By leveraging multiple data sources, we‚Äôve uncovered patterns and trends that reveal how public perception and market behavior are connected.\n\n\n\n\nWe collected data from four main sources to build a comprehensive view of Tesla‚Äôs market behavior:\n\nStock Data (Yahoo Finance)\n\nUsing the yfinance Python library, we pulled stock data from January 1, 2020, to April 29, 2025.\nThe dataset includes core metrics: Open, High, Low, Close prices, and trading Volume.\n\nReddit Data (Reddit API)\n\nWe scraped historical posts from targeted subreddits, focusing on discussions around Tesla and its market influence.\nSentiment analysis was performed using VADER and FinBERT, with scores ranging from -1 (very negative) to +1 (very positive).\n\nStock News (New York Times & The Guardian)\n\nNews articles were scraped dating back to 2008, marking the beginning of Elon Musk‚Äôs role as Tesla‚Äôs CEO.\nKeywords like Tesla helped us filter relevant stories.\n\n\n\n\n\n\nOur analysis was structured to uncover meaningful relationships and insights from the data:\n\nFeature Correlation Matrix: We mapped out relationships between stock indicators and sentiment signals to identify key drivers.\nTechnical Indicators: Using EMAs, MACD, ROC, Bollinger Bands, ATR, and VWAP, we traced market trends and volatility.\nSentiment Analysis: We evaluated how public sentiment, driven by social media and news, aligned with stock price movements.\n\n\n\n\nStock price metrics (Open, High, Low, Close) showed strong internal correlation, reflecting synchronized market movements.\nBollinger Bands effectively signaled optimal entry and exit points in the market.\nNews and Reddit sentiment exhibited limited linear impact, suggesting a more complex, time-lagged relationship with stock prices.\n\n\n\n\n\n\nWe observed clear links between sentiment and price movements, reinforcing the power of public perception in financial markets:\n\nSentiment Impact on Price Movements:\n\nNegative news often led to price drops, while positive sentiment typically drove prices higher.\nThis highlights the crucial role of public perception in influencing market behavior.\n\nTechnical Analysis Validation:\n\nIndicators like Bollinger Bands, ATR, and MACD provided reliable signals for market entry and exit.\nSpikes in ATR and trading volume often marked periods of volatility, signaling high investor activity.\n\n\n\n\n\n\nMoving forward, we plan to: - Implement Machine Learning Models that leverage sentiment and technical indicators to predict market movements with greater accuracy. - Expand the analysis to include other high-impact stocks, enabling broader insights across different sectors. - Explore real-time sentiment tracking for more responsive market predictions.\n\nThis project represents a structured approach to understanding how public mood and market indicators shape stock price movements. By combining technical analysis with sentiment signals, we aim to bring a data-driven perspective to financial market predictions, empowering investors with actionable insights."
  },
  {
    "objectID": "about.html#project-title-market-mood-swings-an-exploratory-analysis-of-stock-price-movements-volatility-and-public-sentiment-on-tesla",
    "href": "about.html#project-title-market-mood-swings-an-exploratory-analysis-of-stock-price-movements-volatility-and-public-sentiment-on-tesla",
    "title": "About This Project",
    "section": "",
    "text": "This project dives deep into the relationship between stock price movements, market volatility, and public sentiment, with a dedicated focus on Tesla Inc.¬†Our goal is to understand how different market indicators, news events, Reddit discussions, and Twitter activity influence stock prices. By leveraging multiple data sources, we‚Äôve uncovered patterns and trends that reveal how public perception and market behavior are connected.\n\n\n\n\nWe collected data from four main sources to build a comprehensive view of Tesla‚Äôs market behavior:\n\nStock Data (Yahoo Finance)\n\nUsing the yfinance Python library, we pulled stock data from January 1, 2020, to April 29, 2025.\nThe dataset includes core metrics: Open, High, Low, Close prices, and trading Volume.\n\nReddit Data (Reddit API)\n\nWe scraped historical posts from targeted subreddits, focusing on discussions around Tesla and its market influence.\nSentiment analysis was performed using VADER and FinBERT, with scores ranging from -1 (very negative) to +1 (very positive).\n\nStock News (New York Times & The Guardian)\n\nNews articles were scraped dating back to 2008, marking the beginning of Elon Musk‚Äôs role as Tesla‚Äôs CEO.\nKeywords like Tesla helped us filter relevant stories.\n\n\n\n\n\n\nOur analysis was structured to uncover meaningful relationships and insights from the data:\n\nFeature Correlation Matrix: We mapped out relationships between stock indicators and sentiment signals to identify key drivers.\nTechnical Indicators: Using EMAs, MACD, ROC, Bollinger Bands, ATR, and VWAP, we traced market trends and volatility.\nSentiment Analysis: We evaluated how public sentiment, driven by social media and news, aligned with stock price movements.\n\n\n\n\nStock price metrics (Open, High, Low, Close) showed strong internal correlation, reflecting synchronized market movements.\nBollinger Bands effectively signaled optimal entry and exit points in the market.\nNews and Reddit sentiment exhibited limited linear impact, suggesting a more complex, time-lagged relationship with stock prices.\n\n\n\n\n\n\nWe observed clear links between sentiment and price movements, reinforcing the power of public perception in financial markets:\n\nSentiment Impact on Price Movements:\n\nNegative news often led to price drops, while positive sentiment typically drove prices higher.\nThis highlights the crucial role of public perception in influencing market behavior.\n\nTechnical Analysis Validation:\n\nIndicators like Bollinger Bands, ATR, and MACD provided reliable signals for market entry and exit.\nSpikes in ATR and trading volume often marked periods of volatility, signaling high investor activity.\n\n\n\n\n\n\nMoving forward, we plan to: - Implement Machine Learning Models that leverage sentiment and technical indicators to predict market movements with greater accuracy. - Expand the analysis to include other high-impact stocks, enabling broader insights across different sectors. - Explore real-time sentiment tracking for more responsive market predictions.\n\nThis project represents a structured approach to understanding how public mood and market indicators shape stock price movements. By combining technical analysis with sentiment signals, we aim to bring a data-driven perspective to financial market predictions, empowering investors with actionable insights."
  },
  {
    "objectID": "data-ingestion.html",
    "href": "data-ingestion.html",
    "title": "üìå Display the Head of the Dataset",
    "section": "",
    "text": "Below is the complete script used for data ingestion and preprocessing. This script handles:\n- Retrieving historical stock price data\n- Collecting sentiment data from Reddit, Guardian, and NYT\n- Merging datasets\n- Adding technical indicators\n\n‚ö†Ô∏è Note: This code is not executed during page rendering because it involves data scraping and long processing times. It is for reference only.\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta, timezone\nimport yfinance as yf\nimport praw\nimport openpyxl\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom functions import dataExploration, calc_indicators, get_reddit_data, get_guardian_data\n\n#SEGMENT: Basic Information\nindex = \"TSLA\"\nexchange = \"^IXIC\"\nyears = 5\nend = datetime.today()\nstart = end - timedelta(days=365 * years)\n\ndf = yf.Ticker(index).history(start=start, end=end)\n\n#SEGMENT: Adding Exchange Data\nexchange_df = yf.Ticker(exchange).history(start=start, end=end)\ndf[\"Exchange_Open\"] = exchange_df[\"Open\"]\ndf[\"Exchange_High\"] = exchange_df[\"High\"]\ndf[\"Exchange_Low\"] = exchange_df[\"Low\"]\ndf[\"Exchange_Close\"] = exchange_df[\"Close\"]\ndf[\"Exchange_Volume\"] = exchange_df[\"Volume\"]\n\ndf = df.reset_index()\ndf[\"Date\"] = df[\"Date\"].dt.date\n\n#SEGMENT: Adding Technical Indicators\ndf = calc_indicators(df, \"Close\")\n\n#SEGMENT: Adding Reddit Data\ndf_reddit = get_reddit_data(years)\ndf_reddit['text'] = df_reddit['title'] + \" \" + df_reddit['selftext'].fillna(\"\")\n\nanalyzer = SentimentIntensityAnalyzer()\ndf_reddit['reddit_sentiment'] = df_reddit['text'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\nreddit_sentiment = df_reddit.groupby('Date').agg({\n    'reddit_sentiment': 'mean',\n    'reddit_score': 'sum'\n}).reset_index()\n\n# SEGMENT: Adding Guardian Data\ndf_guardian = get_guardian_data(years)\ndf_guardian[\"Date\"] = pd.to_datetime(df_guardian[\"Date\"]).dt.date\ndf_guardian['guardian_sentiment'] = df_guardian['GuardianTitle'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\nguardian_sentiment = df_guardian.groupby('Date').agg({\n    'guardian_sentiment': 'mean',\n})\n\n# SEGMENT: Adding NYT Data\ndf_nyt = pd.read_csv('nyt.csv')\ndf_nyt[\"Date\"] = pd.to_datetime(df_nyt[\"pub_date\"]).dt.date\ndf_nyt['text'] = df_nyt['text'].fillna(\"\")\ndf_nyt['nyt_sentiment'] = df_nyt['text'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\nnyt_sentiment = df_nyt.groupby('Date').agg({\n    'nyt_sentiment': 'mean'\n}).reset_index()\n\n# SEGMENT: Merging Data & Preprocessing\ndata = df.merge(reddit_sentiment, on=\"Date\", how=\"left\")\ndata = data.merge(guardian_sentiment, on=\"Date\", how=\"left\")\ndata = data.merge(nyt_sentiment, on=\"Date\", how=\"left\")\n\ndata[\"reddit_sentiment\"] = data[\"reddit_sentiment\"].fillna(0)\ndata[\"reddit_score\"] = data[\"reddit_score\"].fillna(0)\ndata[\"guardian_sentiment\"] = data[\"guardian_sentiment\"].fillna(0)\ndata[\"nyt_sentiment\"] = data[\"nyt_sentiment\"].fillna(0)\n\ndata = data.drop([\"Dividends\", \"Stock Splits\"], axis=1)\ndata = data.fillna(0)\n\n# Save the DataFrame to a CSV file\ndata.to_csv(\"dataBackUp/final_data.csv\")\ndf.to_parquet('dataBackUp/final_data.parquet', engine='pyarrow', index=False)\ndata.to_excel('dataBackUp/final_data.xlsx', index=False)\n\n\nBelow is the preview of the first 5 rows of final_data.csv.\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv(\"final_data.csv\")\ndf.tail()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nDate\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\nExchange_Open\n...\nTP\nVWAP\nTWAP\nCMF\nAdvance_Decline\nCVI\nreddit_sentiment\nreddit_score\nguardian_sentiment\nnyt_sentiment\n\n\n\n\n1251\n1251\n2025-05-01\n280.010010\n290.869995\n279.809998\n280.519989\n99659000\n0.0\n0.0\n17793.140625\n...\n279.322502\n214.398008\n256.582502\n0.120501\n54\n140181406100\n0.511741\n5626.0\n-0.363125\n0.0\n\n\n1252\n1252\n2025-05-02\n284.899994\n294.779999\n279.809998\n287.209991\n114454700\n0.0\n0.0\n17868.759766\n...\n282.802498\n214.453813\n256.470501\n0.132173\n55\n140295860800\n-0.565367\n718.0\n0.000000\n0.0\n\n\n1253\n1253\n2025-05-05\n284.570007\n284.850006\n274.399994\n280.260010\n94618900\n0.0\n0.0\n17817.009766\n...\n286.674995\n214.502488\n257.467001\n0.191804\n54\n140390479700\n0.583733\n543.0\n-0.648600\n0.0\n\n\n1254\n1254\n2025-05-06\n273.109985\n277.730011\n271.350006\n275.350006\n76715800\n0.0\n0.0\n17623.210938\n...\n281.020004\n214.538816\n259.508501\n0.207247\n53\n140467195500\n0.756933\n1431.0\n-0.252300\n0.0\n\n\n1255\n1255\n2025-05-07\n276.815002\n277.920013\n271.010010\n276.220001\n70746362\n0.0\n0.0\n17706.800781\n...\n274.385002\n214.568943\n261.611502\n0.285213\n54\n140537941862\n-0.146800\n916.0\n0.000000\n0.0\n\n\n\n\n5 rows √ó 41 columns"
  },
  {
    "objectID": "data-ingestion.html#data-ingestion-preprocessing-script",
    "href": "data-ingestion.html#data-ingestion-preprocessing-script",
    "title": "üìå Display the Head of the Dataset",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta, timezone\nimport yfinance as yf\nimport praw\nimport openpyxl\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom functions import dataExploration, calc_indicators, get_reddit_data, get_guardian_data\n\n#SEGMENT: Basic Information\nindex = \"TSLA\"\nexchange = \"^IXIC\"\nyears = 5\nend = datetime.today()\nstart = end - timedelta(days=365 * years)\n\ndf = yf.Ticker(index).history(start=start, end=end)\n\n#SEGMENT: Adding Exchange Data\nexchange_df = yf.Ticker(exchange).history(start=start, end=end)\ndf[\"Exchange_Open\"] = exchange_df[\"Open\"]\ndf[\"Exchange_High\"] = exchange_df[\"High\"]\ndf[\"Exchange_Low\"] = exchange_df[\"Low\"]\ndf[\"Exchange_Close\"] = exchange_df[\"Close\"]\ndf[\"Exchange_Volume\"] = exchange_df[\"Volume\"]\n\ndf = df.reset_index()\ndf[\"Date\"] = df[\"Date\"].dt.date\n\n#SEGMENT: Adding Technical Indicators\ndf = calc_indicators(df, \"Close\")\n\n#SEGMENT: Adding Reddit Data\ndf_reddit = get_reddit_data(years)\ndf_reddit['text'] = df_reddit['title'] + \" \" + df_reddit['selftext'].fillna(\"\")\n\nanalyzer = SentimentIntensityAnalyzer()\ndf_reddit['reddit_sentiment'] = df_reddit['text'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\nreddit_sentiment = df_reddit.groupby('Date').agg({\n    'reddit_sentiment': 'mean',\n    'reddit_score': 'sum'\n}).reset_index()\n\n# SEGMENT: Adding Guardian Data\ndf_guardian = get_guardian_data(years)\ndf_guardian[\"Date\"] = pd.to_datetime(df_guardian[\"Date\"]).dt.date\ndf_guardian['guardian_sentiment'] = df_guardian['GuardianTitle'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\nguardian_sentiment = df_guardian.groupby('Date').agg({\n    'guardian_sentiment': 'mean',\n})\n\n# SEGMENT: Adding NYT Data\ndf_nyt = pd.read_csv('nyt.csv')\ndf_nyt[\"Date\"] = pd.to_datetime(df_nyt[\"pub_date\"]).dt.date\ndf_nyt['text'] = df_nyt['text'].fillna(\"\")\ndf_nyt['nyt_sentiment'] = df_nyt['text'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\nnyt_sentiment = df_nyt.groupby('Date').agg({\n    'nyt_sentiment': 'mean'\n}).reset_index()\n\n# SEGMENT: Merging Data & Preprocessing\ndata = df.merge(reddit_sentiment, on=\"Date\", how=\"left\")\ndata = data.merge(guardian_sentiment, on=\"Date\", how=\"left\")\ndata = data.merge(nyt_sentiment, on=\"Date\", how=\"left\")\n\ndata[\"reddit_sentiment\"] = data[\"reddit_sentiment\"].fillna(0)\ndata[\"reddit_score\"] = data[\"reddit_score\"].fillna(0)\ndata[\"guardian_sentiment\"] = data[\"guardian_sentiment\"].fillna(0)\ndata[\"nyt_sentiment\"] = data[\"nyt_sentiment\"].fillna(0)\n\ndata = data.drop([\"Dividends\", \"Stock Splits\"], axis=1)\ndata = data.fillna(0)\n\n# Save the DataFrame to a CSV file\ndata.to_csv(\"dataBackUp/final_data.csv\")\ndf.to_parquet('dataBackUp/final_data.parquet', engine='pyarrow', index=False)\ndata.to_excel('dataBackUp/final_data.xlsx', index=False)\n\n\nBelow is the preview of the first 5 rows of final_data.csv.\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv(\"final_data.csv\")\ndf.tail()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nDate\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\nExchange_Open\n...\nTP\nVWAP\nTWAP\nCMF\nAdvance_Decline\nCVI\nreddit_sentiment\nreddit_score\nguardian_sentiment\nnyt_sentiment\n\n\n\n\n1251\n1251\n2025-05-01\n280.010010\n290.869995\n279.809998\n280.519989\n99659000\n0.0\n0.0\n17793.140625\n...\n279.322502\n214.398008\n256.582502\n0.120501\n54\n140181406100\n0.511741\n5626.0\n-0.363125\n0.0\n\n\n1252\n1252\n2025-05-02\n284.899994\n294.779999\n279.809998\n287.209991\n114454700\n0.0\n0.0\n17868.759766\n...\n282.802498\n214.453813\n256.470501\n0.132173\n55\n140295860800\n-0.565367\n718.0\n0.000000\n0.0\n\n\n1253\n1253\n2025-05-05\n284.570007\n284.850006\n274.399994\n280.260010\n94618900\n0.0\n0.0\n17817.009766\n...\n286.674995\n214.502488\n257.467001\n0.191804\n54\n140390479700\n0.583733\n543.0\n-0.648600\n0.0\n\n\n1254\n1254\n2025-05-06\n273.109985\n277.730011\n271.350006\n275.350006\n76715800\n0.0\n0.0\n17623.210938\n...\n281.020004\n214.538816\n259.508501\n0.207247\n53\n140467195500\n0.756933\n1431.0\n-0.252300\n0.0\n\n\n1255\n1255\n2025-05-07\n276.815002\n277.920013\n271.010010\n276.220001\n70746362\n0.0\n0.0\n17706.800781\n...\n274.385002\n214.568943\n261.611502\n0.285213\n54\n140537941862\n-0.146800\n916.0\n0.000000\n0.0\n\n\n\n\n5 rows √ó 41 columns"
  },
  {
    "objectID": "results/sentiment-analysis.html",
    "href": "results/sentiment-analysis.html",
    "title": "Results & Insights",
    "section": "",
    "text": "Code\nimport warnings\nimport logging\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\ndf = pd.read_csv('final_data.csv')\n\n# SANITY CHECK: Ensure the DataFrame is not empty\nassert not df.empty, \"Error: The loaded DataFrame is empty.\"\n\ndf = df.fillna(0)\ndf[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].shift(1)\n\ndf_base = df.filter(items=['Date', 'High', 'Low', 'Close', 'Volume', \"Open\"])\ndf_technical = df.drop(columns=['reddit_sentiment', 'guardian_sentiment', 'reddit_score', 'nyt_sentiment'])\n\n# SANITY CHECK: Ensure subsets contain the expected columns\nexpected_base_columns = {'Date', 'High', 'Low', 'Close', 'Volume', 'Open'}\nmissing_base_columns = expected_base_columns - set(df_base.columns)\nassert not missing_base_columns, f\"Error: Missing expected columns in df_base: {missing_base_columns}\"\n\n\ndef random_forest_regression(df, dataset_name):\n    # Step 1: Data Preprocessing\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n    \n    # SANITY CHECK: Ensure 'Date' column is successfully converted to datetime\n    assert pd.api.types.is_datetime64_any_dtype(df.index), \"Error: 'Date' column is not properly converted to datetime.\"\n\n    # Create target variable (next day's High)\n    df['Target'] = df['High'].shift(-1)  # Predict next day's High\n\n    # Feature engineering\n    features = df.drop(columns=['High', 'Target'])\n    target = df['Target']\n\n    # Handle missing values after shifting\n    df.dropna(subset=['Target'], inplace=True)\n\n    # Split into train and test (last 30 days + 1 day prediction)\n    split_date = df.index[-31]\n    X_train, X_test = features[:split_date], features[split_date:]\n    y_train, y_test = target[:split_date], target[split_date:]\n\n    # Ensure X_train and X_test are pandas DataFrames with feature names\n    X_train = pd.DataFrame(X_train, columns=features.columns)\n    X_test = pd.DataFrame(X_test, columns=features.columns)\n\n    # Step 2: Train Random Forest Model\n    model_rf = RandomForestRegressor(\n        n_estimators=200,\n        max_depth=15,\n        min_samples_split=10,\n        min_samples_leaf=5,\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model_rf.fit(X_train, y_train)\n\n    # Step 3: Make Predictions\n    y_pred = model_rf.predict(X_test)\n\n    # Step 4: Estimate Confidence Intervals using Prediction Variance\n    # Get predictions from each tree in the forest\n    predictions_per_tree = np.array([tree.predict(X_test) for tree in model_rf.estimators_])\n\n    # Calculate mean and percentiles across trees\n    mean_predictions = predictions_per_tree.mean(axis=0)\n    lower_bound = np.percentile(predictions_per_tree, 5, axis=0)  # 5th percentile for 90% CI\n    upper_bound = np.percentile(predictions_per_tree, 95, axis=0)  # 95th percentile for 90% CI\n    \n    # SANITY CHECK: Ensure predictions have the same length as test data\n    assert len(mean_predictions) == len(y_test), \"Error: Length mismatch between predicted and actual values.\"\n    assert len(lower_bound) == len(y_test), \"Error: Length mismatch between lower bound predictions and actual values.\"\n    assert len(upper_bound) == len(y_test), \"Error: Length mismatch between upper bound predictions and actual values.\"\n\n    # Step 5: Feature Importance\n    feature_importance = pd.DataFrame({\n        'Feature': X_train.columns,\n        'Importance': model_rf.feature_importances_\n    }).sort_values(by='Importance', ascending=False)\n    \n    # SANITY CHECK: Ensure feature importance DataFrame is not empty\n    assert not feature_importance.empty, \"Error: Feature importance DataFrame is empty.\"\n\n\n    # Step 6: Plot Results using Plotly\n    fig1 = go.Figure()\n\n    # Actual Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=y_test[:-1],\n        mode='lines+markers',\n        name='Actual Price',\n        marker=dict(color='blue')\n    ))\n\n    # Predicted Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=mean_predictions[:-1],\n        mode='lines+markers',\n        name='Predicted Price',\n        line=dict(dash='dash'),\n        marker=dict(color='orange')\n    ))\n\n    # Confidence Interval\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1].tolist() + y_test.index[:-1].tolist()[::-1],\n        y=np.concatenate([lower_bound[:-1], upper_bound[:-1][::-1]]),\n        fill='toself',\n        fillcolor='rgba(128, 128, 128, 0.2)',\n        line=dict(color='rgba(255,255,255,0)'),\n        name='90% CI'\n    ))\n\n    # Next Day Prediction\n    last_date = y_test.index[-1] + pd.Timedelta(days=1)\n    fig1.add_trace(go.Scatter(\n        x=[last_date],\n        y=[mean_predictions[-1]],\n        mode='markers',\n        name=f'Next Day Prediction ({mean_predictions[-1]:.2f})',\n        marker=dict(color='red', size=10)\n    ))\n\n    fig1.update_layout(\n        title=f'Tesla Stock High Price Prediction with Random Forest&lt;br&gt;{dataset_name}',\n        xaxis_title='Date',\n        yaxis_title='Price ($)',\n        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.01),\n        template='plotly_white'\n    )\n    fig1.show()\n\n    # Feature Importance Plot\n    top_10_features = feature_importance.head(10)\n    fig2 = go.Figure()\n\n    # ‚û°Ô∏è Optional: Create a gradient color scale (you can remove if you want plain color)\n    colors = \"lightblue\"\n\n    fig2.add_trace(go.Bar(\n        y=top_10_features['Feature'],\n        x=top_10_features['Importance'],\n        orientation='h',\n        marker=dict(\n            color=colors,\n            line=dict(color='black', width=1)\n        ),\n        name='Feature Importance'\n    ))\n\n    # ‚û°Ô∏è Updated Layout Configuration for Figure 2\n    fig2.update_layout(\n        title={\n            'text': f'Top 10 Most Important Features&lt;br&gt;{dataset_name}',\n            'y': 0.95,                 # Pushed higher to avoid overlap\n            'x': 0.5,                  # Centered horizontally\n            'xanchor': 'center',\n            'yanchor': 'top',\n            'font': {'size': 18}       # Slightly larger for clarity\n        },\n        xaxis_title='Feature Importance Score',\n        yaxis_title='Features',\n        yaxis=dict(\n            autorange=\"reversed\",     # Most important features on top\n            tickfont=dict(size=12)    # Larger font for better readability\n        ),\n        margin=dict(l=100, r=40, t=60, b=40),  # Extra space for better readability\n        template='plotly_white',\n        height=500  # Slightly taller for better clarity\n    )\n\n    fig2.show()\n\n    # Step 7: Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(y_test[:-1], mean_predictions[:-1]))\n\n    return {\n        'rmse': rmse,\n        'next_day_prediction': mean_predictions[-1]\n    }\n\n\n\n\n\n#SEGMENT: Analyzing Data with Base Features + Technical Indicators + Sentiment Analysis: Reddit, Guardian, NYT\nprint(\"Analyzing Data with Base Features + Technical Indicators + Sentiment Analysis: Reddit, Guardian, NYT\\n\")\nsentiment = random_forest_regression(df, \"Base Data + Technical Indicators + Sentiment Analysis\")\nprint(sentiment)\n\n\nAnalyzing Data with Base Features + Technical Indicators + Sentiment Analysis: Reddit, Guardian, NYT\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n{'rmse': np.float64(9.771690065263131), 'next_day_prediction': np.float64(279.24225001408746)}"
  },
  {
    "objectID": "results/sentiment-analysis.html#stock-price-prediction-analysis",
    "href": "results/sentiment-analysis.html#stock-price-prediction-analysis",
    "title": "Results & Insights",
    "section": "",
    "text": "In this section, we present the analysis of Tesla‚Äôs stock price prediction using Random Forest Regression. The study is divided into three main parts: 1. Base Features Only - Traditional stock metrics like Open, High, Low, Close, and Volume. 2. Base + Technical Indicators - Including indicators like RSI, MACD, and Bollinger Bands. 3. Base + Technical Indicators + Sentiment Analysis - Combining price metrics, technical analysis, and sentiment data.\n\n\n\nCode\nimport warnings\nimport logging\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\ndf = pd.read_csv('final_data.csv')\n\n# SANITY CHECK: Ensure the DataFrame is not empty\nassert not df.empty, \"Error: The loaded DataFrame is empty.\"\n\ndf = df.fillna(0)\ndf[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].shift(1)\n\ndf_base = df.filter(items=['Date', 'High', 'Low', 'Close', 'Volume', \"Open\"])\ndf_technical = df.drop(columns=['reddit_sentiment', 'guardian_sentiment', 'reddit_score', 'nyt_sentiment'])\n\n# SANITY CHECK: Ensure subsets contain the expected columns\nexpected_base_columns = {'Date', 'High', 'Low', 'Close', 'Volume', 'Open'}\nmissing_base_columns = expected_base_columns - set(df_base.columns)\nassert not missing_base_columns, f\"Error: Missing expected columns in df_base: {missing_base_columns}\"\n\n\ndef random_forest_regression(df, dataset_name):\n    # Step 1: Data Preprocessing\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n    \n    # SANITY CHECK: Ensure 'Date' column is successfully converted to datetime\n    assert pd.api.types.is_datetime64_any_dtype(df.index), \"Error: 'Date' column is not properly converted to datetime.\"\n\n    # Create target variable (next day's High)\n    df['Target'] = df['High'].shift(-1)  # Predict next day's High\n\n    # Feature engineering\n    features = df.drop(columns=['High', 'Target'])\n    target = df['Target']\n\n    # Handle missing values after shifting\n    df.dropna(subset=['Target'], inplace=True)\n\n    # Split into train and test (last 30 days + 1 day prediction)\n    split_date = df.index[-31]\n    X_train, X_test = features[:split_date], features[split_date:]\n    y_train, y_test = target[:split_date], target[split_date:]\n\n    # Ensure X_train and X_test are pandas DataFrames with feature names\n    X_train = pd.DataFrame(X_train, columns=features.columns)\n    X_test = pd.DataFrame(X_test, columns=features.columns)\n\n    # Step 2: Train Random Forest Model\n    model_rf = RandomForestRegressor(\n        n_estimators=200,\n        max_depth=15,\n        min_samples_split=10,\n        min_samples_leaf=5,\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model_rf.fit(X_train, y_train)\n\n    # Step 3: Make Predictions\n    y_pred = model_rf.predict(X_test)\n\n    # Step 4: Estimate Confidence Intervals using Prediction Variance\n    # Get predictions from each tree in the forest\n    predictions_per_tree = np.array([tree.predict(X_test) for tree in model_rf.estimators_])\n\n    # Calculate mean and percentiles across trees\n    mean_predictions = predictions_per_tree.mean(axis=0)\n    lower_bound = np.percentile(predictions_per_tree, 5, axis=0)  # 5th percentile for 90% CI\n    upper_bound = np.percentile(predictions_per_tree, 95, axis=0)  # 95th percentile for 90% CI\n    \n    # SANITY CHECK: Ensure predictions have the same length as test data\n    assert len(mean_predictions) == len(y_test), \"Error: Length mismatch between predicted and actual values.\"\n    assert len(lower_bound) == len(y_test), \"Error: Length mismatch between lower bound predictions and actual values.\"\n    assert len(upper_bound) == len(y_test), \"Error: Length mismatch between upper bound predictions and actual values.\"\n\n    # Step 5: Feature Importance\n    feature_importance = pd.DataFrame({\n        'Feature': X_train.columns,\n        'Importance': model_rf.feature_importances_\n    }).sort_values(by='Importance', ascending=False)\n    \n    # SANITY CHECK: Ensure feature importance DataFrame is not empty\n    assert not feature_importance.empty, \"Error: Feature importance DataFrame is empty.\"\n\n\n    # Step 6: Plot Results using Plotly\n    fig1 = go.Figure()\n\n    # Actual Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=y_test[:-1],\n        mode='lines+markers',\n        name='Actual Price',\n        marker=dict(color='blue')\n    ))\n\n    # Predicted Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=mean_predictions[:-1],\n        mode='lines+markers',\n        name='Predicted Price',\n        line=dict(dash='dash'),\n        marker=dict(color='orange')\n    ))\n\n    # Confidence Interval\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1].tolist() + y_test.index[:-1].tolist()[::-1],\n        y=np.concatenate([lower_bound[:-1], upper_bound[:-1][::-1]]),\n        fill='toself',\n        fillcolor='rgba(128, 128, 128, 0.2)',\n        line=dict(color='rgba(255,255,255,0)'),\n        name='90% CI'\n    ))\n\n    # Next Day Prediction\n    last_date = y_test.index[-1] + pd.Timedelta(days=1)\n    fig1.add_trace(go.Scatter(\n        x=[last_date],\n        y=[mean_predictions[-1]],\n        mode='markers',\n        name=f'Next Day Prediction ({mean_predictions[-1]:.2f})',\n        marker=dict(color='red', size=10)\n    ))\n\n    fig1.update_layout(\n        title=f'Tesla Stock High Price Prediction with Random Forest&lt;br&gt;{dataset_name}',\n        xaxis_title='Date',\n        yaxis_title='Price ($)',\n        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.01),\n        template='plotly_white'\n    )\n    fig1.show()\n\n    # Feature Importance Plot\n    top_10_features = feature_importance.head(10)\n    fig2 = go.Figure()\n\n    fig2.add_trace(go.Bar(\n        y=top_10_features['Feature'],\n        x=top_10_features['Importance'],\n        orientation='h',\n        marker=dict(color='skyblue', line=dict(color='black', width=1)),\n        name='Feature Importance'\n    ))\n\n    fig2.update_layout(\n        title=f'Top 10 Most Important Features&lt;br&gt;{dataset_name}',\n        xaxis_title='Feature Importance Score',\n        yaxis_title='Features',\n        yaxis=dict(autorange=\"reversed\"),\n        template='plotly_white'\n    )\n    fig2.show()\n\n    # Step 7: Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(y_test[:-1], mean_predictions[:-1]))\n\n    return {\n        'rmse': rmse,\n        'next_day_prediction': mean_predictions[-1]\n    }\n\n\n\n\n\n#SEGMENT: Analyzing Data with Base Features + Technical Indicators + Sentiment Analysis: Reddit, Guardian, NYT\nprint(\"Analyzing Data with Base Features + Technical Indicators + Sentiment Analysis: Reddit, Guardian, NYT\\n\")\nsentiment = random_forest_regression(df, \"Base Data + Technical Indicators + Sentiment Analysis\")\nprint(sentiment)\n\n\nAnalyzing Data with Base Features + Technical Indicators + Sentiment Analysis: Reddit, Guardian, NYT\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n{'rmse': np.float64(9.771690065263131), 'next_day_prediction': np.float64(279.24225001408746)}"
  },
  {
    "objectID": "results/sentiment-analysis.html#sentiment",
    "href": "results/sentiment-analysis.html#sentiment",
    "title": "Results & Insights",
    "section": "üîç Sentiment",
    "text": "üîç Sentiment\n\nRMSE: 9.77\nPlot\n\nThe model performs reasonably well in capturing the general trend of the stock prices but struggles slightly during periods of high volatility.\nThe confidence intervals provide valuable insights into the model‚Äôs certainty, allowing users to understand when predictions may be less reliable.\nThe close alignment between the actual and predicted prices suggests that the Random Forest model effectively leverages the base data (OHLCV), technical indicators like RSI, MACD, etc., and sentiment analysis from Reddit, The Guardian, and the New York Times to make accurate predictions.\n\nFeature Importance\n\nThe Close price remains the dominant factor influencing the model‚Äôs predictions, highlighting its critical role in determining future stock price movements.\nThe inclusion of technical indicators like TP and EMA_7 enhances the model‚Äôs predictive power by incorporating additional signals related to price dynamics and momentum.\nWhile sentiment analysis adds value, its impact is secondary to the core OHLCV data and technical indicators, as evidenced by its relatively lower importance¬†scores."
  },
  {
    "objectID": "results/sentiment-analysis.html#sentiment-1",
    "href": "results/sentiment-analysis.html#sentiment-1",
    "title": "Results & Insights",
    "section": "üîç Sentiment",
    "text": "üîç Sentiment\n\nRMSE: 7.95\nPlot\n\nThe model performs reasonably well in capturing the general trend of the stock prices but struggles slightly during periods of high volatility.\nThe confidence intervals provide valuable insights into the model‚Äôs certainty, allowing users to understand when predictions may be less reliable.\nThe close alignment between the actual and predicted prices suggests that the XGBoost model effectively leverages the base data (Open, High, Low, Close, Volume), technical indicators like RSI, MACD, etc., and sentiment analysis from Reddit, The Guardian, and the New York Times to make accurate predictions.\n\nFeature Importance\n\nThe Close price remains the dominant factor influencing the model‚Äôs predictions, highlighting its critical role in determining future stock price movements.\nThe inclusion of technical indicators like EMA_7 enhances the model‚Äôs predictive power by incorporating additional signals related to price dynamics and momentum.\nWhile sentiment analysis adds value, its impact is secondary to the core OHLCV data and technical indicators, as evidenced by its relatively lower importance scores."
  },
  {
    "objectID": "data-ingestion.html#display-the-head-of-the-dataset",
    "href": "data-ingestion.html#display-the-head-of-the-dataset",
    "title": "Data Ingestion & Preprocessing",
    "section": "",
    "text": "Below is the preview of the first 5 rows of final_data.csv.\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_csv(\"final_data.csv\")\n\n# Display the first 5 rows\ndf.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nDate\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\nExchange_Open\n...\nTP\nVWAP\nTWAP\nCMF\nAdvance_Decline\nCVI\nreddit_sentiment\nreddit_score\nguardian_sentiment\nnyt_sentiment\n\n\n\n\n0\n0\n2020-05-08\n52.917999\n54.933334\n52.467335\n54.627998\n241297500\n0.0\n0.0\n9056.889648\n...\n0.000000\n0.000000\n0.0\n0.0\n-1\n241297500\n0.0000\n0.0\n0.0\n0.0000\n\n\n1\n1\n2020-05-11\n52.700668\n54.933334\n52.333332\n54.085999\n247794000\n0.0\n0.0\n9054.910156\n...\n53.736667\n27.225220\n0.0\n0.0\n-2\n489091500\n-0.4767\n28523.0\n0.0\n0.0098\n\n\n2\n2\n2020-05-12\n55.133331\n56.219334\n53.866669\n53.960667\n238603500\n0.0\n0.0\n9225.150391\n...\n53.513333\n35.844814\n0.0\n0.0\n-3\n727695000\n-0.2732\n9.0\n0.0\n0.0000\n\n\n3\n3\n2020-05-13\n54.722000\n55.066666\n50.886665\n52.730667\n285982500\n0.0\n0.0\n9006.049805\n...\n54.795000\n41.191112\n0.0\n0.0\n-4\n1013677500\n0.2263\n2662.0\n0.0\n-0.0816\n\n\n4\n4\n2020-05-14\n52.000000\n53.557335\n50.933334\n53.555332\n205233000\n0.0\n0.0\n8788.040039\n...\n53.351500\n43.238607\n0.0\n0.0\n-3\n1218910500\n0.0000\n8.0\n0.0\n0.0000\n\n\n\n\n5 rows √ó 41 columns"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "üìà Market Mood Swings",
    "section": "",
    "text": "üìà Market Mood Swings\n\nAn Exploratory Analysis of Stock Price Movements, Volatility, and Public Sentiment\n\n\n\n‚úçÔ∏è By Abdul Khayyum, Komal Badgujar, Tanvisha Kose, Vedant Girkar"
  },
  {
    "objectID": "exploratory-analysis.html#feature-correlation-matrix-for-market-mood-analysis",
    "href": "exploratory-analysis.html#feature-correlation-matrix-for-market-mood-analysis",
    "title": "Exploratory Data Analysis",
    "section": "üîç Feature Correlation Matrix for Market Mood Analysis",
    "text": "üîç Feature Correlation Matrix for Market Mood Analysis\nThis heatmap visualizes the correlation between technical indicators, sentiment scores, and Tesla stock metrics.\n\nStrong positive correlations appear among core price indicators (Open, High, Low, Close), and within groups like MACD, EMA, and ROC, indicating internal consistency.\nSentiment features, especially from Reddit and news outlets, show minimal correlation with direct price movement, highlighting their weak linear association.\nThis suggests that non-linear models or lag-aware forecasting might be more suitable for capturing these relationships.\n\n\n\nCode\nimport plotly.express as px\nimport pandas as pd\n\ndf = pd.read_csv(\"final_data.csv\")\n\nif 'Date' in df.columns:\n    df['Date'] = pd.to_datetime(df['Date'])\n\ndf_numeric = df.select_dtypes(include=['float64', 'int64'])\n\ncorrelation_matrix = df_numeric.corr()\n\nfig = px.imshow(\n    correlation_matrix,\n    labels=dict(color=\"Correlation\"),\n    x=correlation_matrix.columns,\n    y=correlation_matrix.columns,\n    color_continuous_scale='RdBu'\n)\n\nfig.update_layout(title=None)\n\nfig.update_layout(\n    coloraxis_colorbar=dict(\n        x=0.75,\n        thickness=15,\n        title=dict(\n            text=\"Correlation\",\n            font=dict(size=14)  # &lt;-- Corrected property path\n        ),\n        tickfont=dict(size=12)\n    )\n)\n\nfig.update_xaxes(linewidth=1, linecolor='black', mirror=True, ticks='inside', showline=True)\nfig.update_yaxes(linewidth=1, linecolor='black', mirror=True, ticks='inside', showline=True)\n\nfig.show()"
  },
  {
    "objectID": "exploratory-analysis.html#nyt-sentiment-vs.-macd-indicator-20242025",
    "href": "exploratory-analysis.html#nyt-sentiment-vs.-macd-indicator-20242025",
    "title": "Exploratory Data Analysis",
    "section": "üîç NYT Sentiment vs.¬†MACD Indicator (2024‚Äì2025)",
    "text": "üîç NYT Sentiment vs.¬†MACD Indicator (2024‚Äì2025)\nThis dual-axis time series chart compares smoothed New York Times sentiment scores against Tesla‚Äôs MACD indicator.\n\nA moderate positive correlation (0.38) is observed, especially during major events such as Q1 2025‚Äôs market dip and recovery.\nWhile sentiment occasionally aligns with trend shifts, it often lags behind technical indicators, implying that news sentiment alone may not be a reliable standalone signal but can add explanatory value when combined with technical data.\n\n\n\nCode\nimport matplotlib\nimport logging\nimport pandas as pd\nimport plotly.graph_objects as go\nimport numpy as np\n\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\npd.options.mode.chained_assignment = None  # &lt;--- This suppresses the SettingWithCopyWarning\n\ndata = pd.read_csv('final_data.csv', parse_dates=['Date'])\ndata.sort_values('Date', inplace=True)\n\none_year_data = data[data['Date'] &gt;= (data['Date'].max() - pd.DateOffset(years=1))].copy()\n\none_year_data.loc[:, 'nyt_sentiment_smooth'] = one_year_data['nyt_sentiment'].rolling(window=7).mean()\none_year_data.loc[:, 'macd_smooth'] = one_year_data['MACD'].rolling(window=7).mean()\n\ncorrelation = one_year_data[['nyt_sentiment_smooth', 'macd_smooth']].corr().iloc[0, 1]\n\n# --- Visualization ---\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=one_year_data['Date'], \n    y=one_year_data['nyt_sentiment_smooth'], \n    mode='lines', \n    name='NYT Sentiment (Smoothed)', \n    line=dict(color='blue')\n))\n\nfig.add_trace(go.Scatter(\n    x=one_year_data['Date'], \n    y=one_year_data['macd_smooth'], \n    mode='lines', \n    name='MACD (Smoothed)', \n    line=dict(color='green'),\n    yaxis='y2'\n))\n\nfig.update_layout(\n    title=f\"NYT Sentiment vs. MACD Over the Last Year (Correlation: {correlation:.2f})\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"NYT Sentiment\",\n    yaxis2=dict(title=\"MACD\", overlaying='y', side='right'),\n    legend_title=\"Metrics\",\n    template=\"plotly_white\"\n)\nfig.show()"
  },
  {
    "objectID": "exploratory-analysis.html#tesla-volatility-with-bollinger-bands-atr-reddit-sentiment-april-2025",
    "href": "exploratory-analysis.html#tesla-volatility-with-bollinger-bands-atr-reddit-sentiment-april-2025",
    "title": "Exploratory Data Analysis",
    "section": "üîç Tesla Volatility with Bollinger Bands, ATR & Reddit Sentiment (April 2025)",
    "text": "üîç Tesla Volatility with Bollinger Bands, ATR & Reddit Sentiment (April 2025)\nThis multi-panel chart integrates price candlesticks with Bollinger Bands, ATR, and Reddit sentiment.\n\nTesla‚Äôs bounce from the lower band in early April reflects a textbook volatility rebound.\nElevated ATR and volume around April 9 indicate market stress, potentially linked to earnings or macroeconomic developments.\nReddit sentiment trails the actual price action, suggesting that while social signals are useful, they are most effective when interpreted alongside traditional technical metrics.\n\n\n\nCode\nimport matplotlib\nimport logging\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\nimport pandas as pd\nimport mplfinance as mpf\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"final_data.csv\")\ndf['Date'] = pd.to_datetime(df['Date'])\ndf = df.tail(30)\ndf_candle = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']].copy()\ndf_candle.set_index('Date', inplace=True)\n\ncustom_style = mpf.make_mpf_style(\n    base_mpf_style='classic',\n    rc={\n        'font.family': 'Times New Roman',\n        'axes.labelsize': 13,\n        'xtick.labelsize': 11,\n        'ytick.labelsize': 11\n    },\n    marketcolors=mpf.make_marketcolors(\n        up='#2ca02c',\n        down='#d62728',\n        edge='inherit',\n        wick='gray',\n        volume='lightblue'\n    ),\n    gridcolor='lightgray',\n    gridstyle='--',\n    facecolor='white',\n    figcolor='white'\n)\n\napds = [\n    mpf.make_addplot(df['Bollinger_Upper_Band'], color='darkgreen', linestyle='--', width=1.2),\n    mpf.make_addplot(df['Bollinger_Lower_Band'], color='darkred', linestyle='--', width=1.2),\n    mpf.make_addplot(df['ATR'], panel=1, color='purple', width=1.3, ylabel='ATR'),\n    mpf.make_addplot(df['reddit_sentiment'], panel=2, color='navy', width=1.3, ylabel='Reddit Sentiment')\n]\n\nfig, axes = mpf.plot(\n    df_candle,\n    type='candle',\n    style=custom_style,\n    volume=True,\n    addplot=apds,\n    ylabel='Price ($)',\n    ylabel_lower='Volume',\n    panel_ratios=(3, 1, 1),\n    figratio=(16, 9),\n    figscale=1.5,\n    returnfig=True,\n    datetime_format='%b %d'\n)\n\naxes[0].text(\n    0.5, 0.95,\n    'Tesla Volatility with Bollinger Bands, ATR & Reddit Sentiment',\n    transform=axes[0].transAxes,\n    fontsize=14,\n    fontweight='bold',\n    fontfamily='Times New Roman',\n    verticalalignment='top',\n    horizontalalignment='center'\n)\nplt.show()"
  },
  {
    "objectID": "exploratory-analysis.html#tesla-volatility-with-bollinger-bands-atr-reddit-sentiment-april-2025-1",
    "href": "exploratory-analysis.html#tesla-volatility-with-bollinger-bands-atr-reddit-sentiment-april-2025-1",
    "title": "Exploratory Analysis",
    "section": "üîç Tesla Volatility with Bollinger Bands, ATR & Reddit Sentiment (April 2025)",
    "text": "üîç Tesla Volatility with Bollinger Bands, ATR & Reddit Sentiment (April 2025)\nThis multi-panel chart integrates price candlesticks with Bollinger Bands, ATR, and Reddit sentiment.\n\nTesla‚Äôs bounce from the lower band in early April reflects a textbook volatility rebound.\nElevated ATR and volume around April 9 indicate market stress, potentially linked to earnings or macroeconomic developments.\nReddit sentiment trails the actual price action, suggesting that while social signals are useful, they are most effective when interpreted alongside traditional technical metrics.\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom matplotlib.patches import Rectangle\n\n# Load dataset\ndf = pd.read_csv(\"final_data.csv\")\ndf.columns = df.columns.str.strip()\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Filter for March 24‚Äì27, 2025\nstart_date = pd.to_datetime(\"2025-03-24\")\nend_date = start_date + pd.Timedelta(days=3)\nmarch_df = df[(df['Date'] &gt;= start_date) & (df['Date'] &lt;= end_date)].copy()\nmarch_df['Daily_Return_%'] = march_df['Close'].pct_change() * 100\n\n# Create 3 subplots\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10), sharex=True, gridspec_kw={'height_ratios': [3, 1, 1]})\n\n# --- Candlestick Plot ---\nfor _, row in march_df.iterrows():\n    date_num = mdates.date2num(row['Date'])\n    color = 'green' if row['Close'] &gt;= row['Open'] else 'red'\n\n    # Wick\n    ax1.plot([row['Date'], row['Date']], [row['Low'], row['High']], color='black')\n\n    # Body\n    rect = Rectangle((date_num - 0.2, min(row['Open'], row['Close'])),\n                     0.4,\n                     abs(row['Close'] - row['Open']),\n                     color=color)\n    ax1.add_patch(rect)\n\nax1.set_ylabel(\"Price ($)\")\nax1.set_title(\"Tesla: Return %, and NYT Sentiment (March 24‚Äì27, 2025)\")\nax1.grid(True)\n\n# --- Daily Return Plot ---\nax2.bar(march_df['Date'], march_df['Daily_Return_%'], color='skyblue')\nax2.axhline(0, color='black', linestyle='--')\nax2.set_ylabel(\"Return (%)\")\nax2.grid(True)\n\n# --- FinBERT Sentiment Plot ---\nax3.plot(march_df['Date'], march_df['nyt_sentiment'], marker='o', linestyle='--', color='red')\nax3.set_ylabel(\"NEW YORK TIME Sentiment\")\nax3.set_xlabel(\"Date\")\nax3.grid(True)\n\n# Format x-axis\nax3.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\nfig.autofmt_xdate()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "results/base-features.html#tesla-stock-high-price-prediction-with-ranom-forest",
    "href": "results/base-features.html#tesla-stock-high-price-prediction-with-ranom-forest",
    "title": "Results & Insights",
    "section": "",
    "text": "Code\nimport warnings\nimport logging\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\ndf = pd.read_csv('final_data.csv')\n\n# SANITY CHECK: Ensure the DataFrame is not empty\nassert not df.empty, \"Error: The loaded DataFrame is empty.\"\n\ndf = df.fillna(0)\ndf[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].shift(1)\n\ndf_base = df.filter(items=['Date', 'High', 'Low', 'Close', 'Volume', \"Open\"])\ndf_technical = df.drop(columns=['reddit_sentiment', 'guardian_sentiment', 'reddit_score', 'nyt_sentiment'])\n\n# SANITY CHECK: Ensure subsets contain the expected columns\nexpected_base_columns = {'Date', 'High', 'Low', 'Close', 'Volume', 'Open'}\nmissing_base_columns = expected_base_columns - set(df_base.columns)\nassert not missing_base_columns, f\"Error: Missing expected columns in df_base: {missing_base_columns}\"\n\n\ndef random_forest_regression(df, dataset_name):\n    # Step 1: Data Preprocessing\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n    \n    # SANITY CHECK: Ensure 'Date' column is successfully converted to datetime\n    assert pd.api.types.is_datetime64_any_dtype(df.index), \"Error: 'Date' column is not properly converted to datetime.\"\n\n    # Create target variable (next day's High)\n    df['Target'] = df['High'].shift(-1)  # Predict next day's High\n\n    # Feature engineering\n    features = df.drop(columns=['High', 'Target'])\n    target = df['Target']\n\n    # Handle missing values after shifting\n    df.dropna(subset=['Target'], inplace=True)\n\n    # Split into train and test (last 30 days + 1 day prediction)\n    split_date = df.index[-31]\n    X_train, X_test = features[:split_date], features[split_date:]\n    y_train, y_test = target[:split_date], target[split_date:]\n\n    # Ensure X_train and X_test are pandas DataFrames with feature names\n    X_train = pd.DataFrame(X_train, columns=features.columns)\n    X_test = pd.DataFrame(X_test, columns=features.columns)\n\n    # Step 2: Train Random Forest Model\n    model_rf = RandomForestRegressor(\n        n_estimators=200,\n        max_depth=15,\n        min_samples_split=10,\n        min_samples_leaf=5,\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model_rf.fit(X_train, y_train)\n\n    # Step 3: Make Predictions\n    y_pred = model_rf.predict(X_test)\n\n    # Step 4: Estimate Confidence Intervals using Prediction Variance\n    # Get predictions from each tree in the forest\n    predictions_per_tree = np.array([tree.predict(X_test) for tree in model_rf.estimators_])\n\n    # Calculate mean and percentiles across trees\n    mean_predictions = predictions_per_tree.mean(axis=0)\n    lower_bound = np.percentile(predictions_per_tree, 5, axis=0)  # 5th percentile for 90% CI\n    upper_bound = np.percentile(predictions_per_tree, 95, axis=0)  # 95th percentile for 90% CI\n    \n    # SANITY CHECK: Ensure predictions have the same length as test data\n    assert len(mean_predictions) == len(y_test), \"Error: Length mismatch between predicted and actual values.\"\n    assert len(lower_bound) == len(y_test), \"Error: Length mismatch between lower bound predictions and actual values.\"\n    assert len(upper_bound) == len(y_test), \"Error: Length mismatch between upper bound predictions and actual values.\"\n\n    # Step 5: Feature Importance\n    feature_importance = pd.DataFrame({\n        'Feature': X_train.columns,\n        'Importance': model_rf.feature_importances_\n    }).sort_values(by='Importance', ascending=False)\n    \n    # SANITY CHECK: Ensure feature importance DataFrame is not empty\n    assert not feature_importance.empty, \"Error: Feature importance DataFrame is empty.\"\n\n\n    # Step 6: Plot Results using Plotly\n    fig1 = go.Figure()\n\n    # Actual Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=y_test[:-1],\n        mode='lines+markers',\n        name='Actual Price',\n        marker=dict(color='blue')\n    ))\n\n    # Predicted Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=mean_predictions[:-1],\n        mode='lines+markers',\n        name='Predicted Price',\n        line=dict(dash='dash'),\n        marker=dict(color='orange')\n    ))\n\n    # Confidence Interval\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1].tolist() + y_test.index[:-1].tolist()[::-1],\n        y=np.concatenate([lower_bound[:-1], upper_bound[:-1][::-1]]),\n        fill='toself',\n        fillcolor='rgba(128, 128, 128, 0.2)',\n        line=dict(color='rgba(255,255,255,0)'),\n        name='90% CI'\n    ))\n\n    # Next Day Prediction\n    last_date = y_test.index[-1] + pd.Timedelta(days=1)\n    fig1.add_trace(go.Scatter(\n        x=[last_date],\n        y=[mean_predictions[-1]],\n        mode='markers',\n        name=f'Next Day Prediction ({mean_predictions[-1]:.2f})',\n        marker=dict(color='red', size=10)\n    ))\n\n    fig1.update_layout(\n        title=f'Tesla Stock High Price Prediction with Random Forest&lt;br&gt;{dataset_name}',\n        xaxis_title='Date',\n        yaxis_title='Price ($)',\n        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.01),\n        template='plotly_white'\n    )\n    fig1.show()\n\n    # Feature Importance Plot\n    top_10_features = feature_importance.head(10)\n    fig2 = go.Figure()\n\n    fig2.add_trace(go.Bar(\n        y=top_10_features['Feature'],\n        x=top_10_features['Importance'],\n        orientation='h',\n        marker=dict(color='skyblue', line=dict(color='black', width=1)),\n        name='Feature Importance'\n    ))\n\n    fig2.update_layout(\n        title=f'Top 10 Most Important Features&lt;br&gt;{dataset_name}',\n        xaxis_title='Feature Importance Score',\n        yaxis_title='Features',\n        yaxis=dict(autorange=\"reversed\"),\n        template='plotly_white'\n    )\n    fig2.show()\n\n    # Step 7: Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(y_test[:-1], mean_predictions[:-1]))\n\n    return {\n        'rmse': rmse,\n        'next_day_prediction': mean_predictions[-1]\n    }\n\n\n\n#SEGMENT: Analyzing Data with Only the Base Features : Open, High, Low, Close, Volume\nprint(\"Analyzing Data with Only the Base Features : Open, High, Low, Close, Volume\\n\")\nbase = random_forest_regression(df_base, \"Base Data\")\nprint(base)\n\n\nAnalyzing Data with Only the Base Features : Open, High, Low, Close, Volume\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n{'rmse': np.float64(13.538690981605868), 'next_day_prediction': np.float64(281.5759534159752)}"
  },
  {
    "objectID": "results/base-features.html#tesla-stock-high-price-prediction-with",
    "href": "results/base-features.html#tesla-stock-high-price-prediction-with",
    "title": "Results & Insights",
    "section": "üöÄ Tesla Stock High Price Prediction With",
    "text": "üöÄ Tesla Stock High Price Prediction With\n\n\nCode\nimport warnings\nimport logging\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nimport plotly.graph_objects as go\n\ndf = pd.read_csv('final_data.csv')\n\n# SANITY CHECK: Ensure the DataFrame is not empty\nassert not df.empty, \"Error: The loaded DataFrame is empty.\"\n\ndf[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].shift(1)\ndf = df.fillna(0)\n# SANITY CHECK: Verify no missing values after shifting\nassert df[['Open', 'High', 'Low', 'Close', 'Volume']].isnull().sum().sum() == 0, \"Error: Missing values found after shifting.\"\n\n\ndf_base = df.filter(items=['Date', 'High', 'Low', 'Close', 'Volume', \"Open\"])\n# SANITY CHECK: Ensure subsets contain the expected columns\nexpected_base_columns = {'Date', 'High', 'Low', 'Close', 'Volume', 'Open'}\nmissing_base_columns = expected_base_columns - set(df_base.columns)\nassert not missing_base_columns, f\"Error: Missing expected columns in df_base: {missing_base_columns}\"\n\ndf_technical = df.drop(columns=['reddit_sentiment', 'guardian_sentiment', 'reddit_score', 'nyt_sentiment'])\n# SANITY CHECK: Ensure subsets contain the expected columns\nexpected_technical_columns = set(df.columns) - {'reddit_sentiment', 'guardian_sentiment', 'reddit_score', 'nyt_sentiment'}\nmissing_technical_columns = expected_technical_columns - set(df_technical.columns)\nassert not missing_technical_columns, f\"Error: Missing expected columns in df_technical: {missing_technical_columns}\"\n\ndef xgboost_regressor(df, dataset_name):\n    # Preprocess data\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n    \n    # SANITY CHECK: Ensure 'Date' column is successfully converted to datetime\n    assert pd.api.types.is_datetime64_any_dtype(df.index), \"Error: 'Date' column is not properly converted to datetime.\"\n\n    # Create target variable (next day's High)\n    df['Target'] = df['High'].shift(-1)\n    df.dropna(subset=['Target'], inplace=True)\n\n    # SANITY CHECK: Ensure 'Target' column exists and has no missing values\n    assert 'Target' in df.columns, \"Error: 'Target' column not found in the DataFrame.\"\n    assert df['Target'].isnull().sum() == 0, \"Error: Missing values found in 'Target' column.\"\n    \n    # Features and target\n    features = df.drop(columns=['High', 'Target'])\n    target = df['Target']\n\n    # Split into train and test (last 30 days + 1 day prediction)\n    split_date = df.index[-31]\n    X_train, X_test = features[:split_date], features[split_date:]\n    y_train, y_test = target[:split_date], target[split_date:]\n\n    # Train mean prediction model\n    model_mean = xgb.XGBRegressor(\n        n_estimators=200,\n        learning_rate=0.1,\n        max_depth=5,\n        subsample=0.8,\n        colsample_bytree=0.7,\n        reg_lambda=5.0,  # Increase L2 regularization\n        reg_alpha=2.0,  # Add L1 regularization\n        gamma=0.5,\n        random_state=42\n    )\n    model_mean.fit(X_train, y_train)\n\n    # Train quantile models for 95% CI\n    base_params = model_mean.get_params()\n    base_params.pop('objective', None)  # Remove default objective\n\n    model_lower = xgb.XGBRegressor(\n        objective='reg:quantileerror',\n        quantile_alpha=0.1,  # Correct parameter name\n        **base_params\n    )\n    model_upper = xgb.XGBRegressor(\n        objective='reg:quantileerror',\n        quantile_alpha=0.90,  # Correct parameter name\n        **base_params\n    )\n\n    model_lower.fit(X_train, y_train)\n    model_upper.fit(X_train, y_train)\n\n    # Predictions\n    y_pred_mean = model_mean.predict(X_test)\n    y_pred_lower = model_lower.predict(X_test)\n    y_pred_upper = model_upper.predict(X_test)\n    \n    # SANITY CHECK: Ensure predictions have the same length as test data\n    assert len(y_pred_mean) == len(y_test), \"Error: Length mismatch between predicted and actual values.\"\n    assert len(y_pred_lower) == len(y_test), \"Error: Length mismatch between lower bound predictions and actual values.\"\n    assert len(y_pred_upper) == len(y_test), \"Error: Length mismatch between upper bound predictions and actual values.\"\n\n    # Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(y_test[:-1], y_pred_mean[:-1]))\n\n    # Feature importance\n    importance = pd.DataFrame({\n        'Feature': features.columns,\n        'Importance': model_mean.feature_importances_\n    }).sort_values(by='Importance', ascending=False)\n    \n    # SANITY CHECK: Ensure feature importance DataFrame is not empty\n    assert not importance.empty, \"Error: Feature importance DataFrame is empty.\"\n\n    # Plot results using Plotly\n    fig1 = go.Figure()\n\n    # Actual Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=y_test[:-1],\n        mode='lines+markers',\n        name='Actual Price',\n        marker=dict(color='blue')\n    ))\n\n    # Predicted Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=y_pred_mean[:-1],\n        mode='lines+markers',\n        name='Predicted Price',\n        line=dict(dash='dash'),\n        marker=dict(color='orange')\n    ))\n\n    # Confidence Interval\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1].tolist() + y_test.index[:-1].tolist()[::-1],\n        y=np.concatenate([y_pred_lower[:-1], y_pred_upper[:-1][::-1]]),\n        fill='toself',\n        fillcolor='rgba(128, 128, 128, 0.2)',\n        line=dict(color='rgba(255,255,255,0)'),\n        name='90% CI'\n    ))\n\n    # Next Day Prediction\n    last_date = y_test.index[-1] + pd.Timedelta(days=1)\n    fig1.add_trace(go.Scatter(\n        x=[last_date],\n        y=[y_pred_mean[-1]],\n        mode='markers',\n        name=f'Next Day Prediction ({y_pred_mean[-1]:.2f})',\n        marker=dict(color='red', size=10)\n    ))\n\n    fig1.update_layout(\n        title=f'Tesla Stock High Price Prediction with XGBoost&lt;br&gt;{dataset_name}',\n        xaxis_title='Date',\n        yaxis_title='Price ($)',\n        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.01),\n        template='plotly_white'\n    )\n    fig1.show()\n\n    # Feature Importance Plot\n    top_10_features = importance.head(10)\n    fig2 = go.Figure()\n\n    fig2.add_trace(go.Bar(\n        y=top_10_features['Feature'],\n        x=top_10_features['Importance'],\n        orientation='h',\n        marker=dict(color='skyblue', line=dict(color='black', width=1)),\n        name='Feature Importance'\n    ))\n\n    fig2.update_layout(\n        title=f'Top 10 Most Important Features&lt;br&gt;{dataset_name}',\n        xaxis_title='Feature Importance Score',\n        yaxis_title='Features',\n        yaxis=dict(autorange=\"reversed\"),\n        template='plotly_white'\n    )\n    fig2.show()\n\n    return {\n        'rmse': rmse,\n        'feature_importance': importance,\n        'next_day_prediction': y_pred_mean[-1]\n    }\n\n#SEGMENT: Analyzing Data with Only the Base Features : Open, High, Low, Close, Volume\nprint(\"Analyzing Data with Only the Base Features : Open, High, Low, Close, Volume\\n\")\nbase = xgboost_regressor(df_base, \"Base Data\")\nprint(base)\n\n\nAnalyzing Data with Only the Base Features : Open, High, Low, Close, Volume\n\n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n{'rmse': np.float64(12.460339799782533), 'feature_importance':   Feature  Importance\n1   Close    0.635840\n0     Low    0.195846\n3    Open    0.166153\n2  Volume    0.002160, 'next_day_prediction': np.float32(292.05157)}"
  },
  {
    "objectID": "results/base-features.html#tesla-stock-high-price-prediction-with-random-forest",
    "href": "results/base-features.html#tesla-stock-high-price-prediction-with-random-forest",
    "title": "Results & Insights",
    "section": "",
    "text": "Code\nimport warnings\nimport logging\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\ndf = pd.read_csv('final_data.csv')\n\n# SANITY CHECK: Ensure the DataFrame is not empty\nassert not df.empty, \"Error: The loaded DataFrame is empty.\"\n\ndf = df.fillna(0)\ndf[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].shift(1)\n\ndf_base = df.filter(items=['Date', 'High', 'Low', 'Close', 'Volume', \"Open\"])\ndf_technical = df.drop(columns=['reddit_sentiment', 'guardian_sentiment', 'reddit_score', 'nyt_sentiment'])\n\n# SANITY CHECK: Ensure subsets contain the expected columns\nexpected_base_columns = {'Date', 'High', 'Low', 'Close', 'Volume', 'Open'}\nmissing_base_columns = expected_base_columns - set(df_base.columns)\nassert not missing_base_columns, f\"Error: Missing expected columns in df_base: {missing_base_columns}\"\n\n\ndef random_forest_regression(df, dataset_name):\n    # Step 1: Data Preprocessing\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n    \n    # SANITY CHECK: Ensure 'Date' column is successfully converted to datetime\n    assert pd.api.types.is_datetime64_any_dtype(df.index), \"Error: 'Date' column is not properly converted to datetime.\"\n\n    # Create target variable (next day's High)\n    df['Target'] = df['High'].shift(-1)  # Predict next day's High\n\n    # Feature engineering\n    features = df.drop(columns=['High', 'Target'])\n    target = df['Target']\n\n    # Handle missing values after shifting\n    df.dropna(subset=['Target'], inplace=True)\n\n    # Split into train and test (last 30 days + 1 day prediction)\n    split_date = df.index[-31]\n    X_train, X_test = features[:split_date], features[split_date:]\n    y_train, y_test = target[:split_date], target[split_date:]\n\n    # Ensure X_train and X_test are pandas DataFrames with feature names\n    X_train = pd.DataFrame(X_train, columns=features.columns)\n    X_test = pd.DataFrame(X_test, columns=features.columns)\n\n    # Step 2: Train Random Forest Model\n    model_rf = RandomForestRegressor(\n        n_estimators=200,\n        max_depth=15,\n        min_samples_split=10,\n        min_samples_leaf=5,\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model_rf.fit(X_train, y_train)\n\n    # Step 3: Make Predictions\n    y_pred = model_rf.predict(X_test)\n\n    # Step 4: Estimate Confidence Intervals using Prediction Variance\n    # Get predictions from each tree in the forest\n    predictions_per_tree = np.array([tree.predict(X_test) for tree in model_rf.estimators_])\n\n    # Calculate mean and percentiles across trees\n    mean_predictions = predictions_per_tree.mean(axis=0)\n    lower_bound = np.percentile(predictions_per_tree, 5, axis=0)  # 5th percentile for 90% CI\n    upper_bound = np.percentile(predictions_per_tree, 95, axis=0)  # 95th percentile for 90% CI\n    \n    # SANITY CHECK: Ensure predictions have the same length as test data\n    assert len(mean_predictions) == len(y_test), \"Error: Length mismatch between predicted and actual values.\"\n    assert len(lower_bound) == len(y_test), \"Error: Length mismatch between lower bound predictions and actual values.\"\n    assert len(upper_bound) == len(y_test), \"Error: Length mismatch between upper bound predictions and actual values.\"\n\n    # Step 5: Feature Importance\n    feature_importance = pd.DataFrame({\n        'Feature': X_train.columns,\n        'Importance': model_rf.feature_importances_\n    }).sort_values(by='Importance', ascending=False)\n    \n    # SANITY CHECK: Ensure feature importance DataFrame is not empty\n    assert not feature_importance.empty, \"Error: Feature importance DataFrame is empty.\"\n\n\n    # Step 6: Plot Results using Plotly\n    fig1 = go.Figure()\n\n    # Actual Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=y_test[:-1],\n        mode='lines+markers',\n        name='Actual Price',\n        marker=dict(color='blue')\n    ))\n\n    # Predicted Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=mean_predictions[:-1],\n        mode='lines+markers',\n        name='Predicted Price',\n        line=dict(dash='dash'),\n        marker=dict(color='orange')\n    ))\n\n    # Confidence Interval\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1].tolist() + y_test.index[:-1].tolist()[::-1],\n        y=np.concatenate([lower_bound[:-1], upper_bound[:-1][::-1]]),\n        fill='toself',\n        fillcolor='rgba(128, 128, 128, 0.2)',\n        line=dict(color='rgba(255,255,255,0)'),\n        name='90% CI'\n    ))\n\n    # Next Day Prediction\n    last_date = y_test.index[-1] + pd.Timedelta(days=1)\n    fig1.add_trace(go.Scatter(\n        x=[last_date],\n        y=[mean_predictions[-1]],\n        mode='markers',\n        name=f'Next Day Prediction ({mean_predictions[-1]:.2f})',\n        marker=dict(color='red', size=10)\n    ))\n\n    fig1.update_layout(\n        title=f'Tesla Stock High Price Prediction with Random Forest&lt;br&gt;{dataset_name}',\n        xaxis_title='Date',\n        yaxis_title='Price ($)',\n        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.01),\n        template='plotly_white'\n    )\n    fig1.show()\n\n    # Feature Importance Plot\n    top_10_features = feature_importance.head(10).sort_values(by='Importance', ascending=True)\n\n    # Optional: Create a gradient color scale (you can remove if you want plain color)\n    colors = ['#1f77b4', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', '#ff7f0e']\n\n    fig2 = go.Figure()\n\n    fig2.add_trace(go.Bar(\n        y=top_10_features['Feature'],\n        x=top_10_features['Importance'],\n        orientation='h',\n        marker=dict(\n            color=\"lightblue\",\n            line=dict(color='black', width=1)\n        ),\n        name='Feature Importance',\n        hovertemplate='&lt;b&gt;%{y}&lt;/b&gt;: %{x:.4f}'  # Displays the score on hover\n    ))\n\n    # ‚û°Ô∏è Updated Layout Configuration for Figure 2\n    fig2.update_layout(\n        title={\n            'text': f'Top 10 Most Important Features&lt;br&gt;{dataset_name}',\n            'y': 0.95,                 # Pushed higher to avoid overlap\n            'x': 0.5,                  # Centered horizontally\n            'xanchor': 'center',\n            'yanchor': 'top',\n            'font': {'size': 18}       # Slightly larger for clarity\n        },\n        xaxis_title='Feature Importance Score',\n        yaxis_title='Features',\n        yaxis=dict(\n            autorange=False,\n            tickfont=dict(size=12),\n            categoryorder='total ascending'  # Ensures the most important is on top\n        ),\n        margin=dict(l=100, r=40, t=60, b=40),  # Extra space for better readability\n        template='plotly_white',\n        height=500  # Slightly taller for better clarity\n    )\n\n    fig2.show()\n\n    # Step 7: Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(y_test[:-1], mean_predictions[:-1]))\n\n    return {\n        'rmse': rmse,\n        'next_day_prediction': mean_predictions[-1]\n    }\n\n\n\n#SEGMENT: Analyzing Data with Only the Base Features : Open, High, Low, Close, Volume\nprint(\"Analyzing Data with Only the Base Features : Open, High, Low, Close, Volume\\n\")\nbase = random_forest_regression(df_base, \"Base Data\")\nprint(base)\n\n\nAnalyzing Data with Only the Base Features : Open, High, Low, Close, Volume\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n{'rmse': np.float64(13.538690981605868), 'next_day_prediction': np.float64(281.5759534159752)}"
  },
  {
    "objectID": "results/base-features.html#tesla-stock-high-price-prediction-with-xgboost",
    "href": "results/base-features.html#tesla-stock-high-price-prediction-with-xgboost",
    "title": "Results & Insights",
    "section": "üöÄ Tesla Stock High Price Prediction With XGBoost",
    "text": "üöÄ Tesla Stock High Price Prediction With XGBoost\n\n\nCode\nimport warnings\nimport logging\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nimport plotly.graph_objects as go\n\ndf = pd.read_csv('final_data.csv')\n\n# SANITY CHECK: Ensure the DataFrame is not empty\nassert not df.empty, \"Error: The loaded DataFrame is empty.\"\n\ndf[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].shift(1)\ndf = df.fillna(0)\n# SANITY CHECK: Verify no missing values after shifting\nassert df[['Open', 'High', 'Low', 'Close', 'Volume']].isnull().sum().sum() == 0, \"Error: Missing values found after shifting.\"\n\n\ndf_base = df.filter(items=['Date', 'High', 'Low', 'Close', 'Volume', \"Open\"])\n# SANITY CHECK: Ensure subsets contain the expected columns\nexpected_base_columns = {'Date', 'High', 'Low', 'Close', 'Volume', 'Open'}\nmissing_base_columns = expected_base_columns - set(df_base.columns)\nassert not missing_base_columns, f\"Error: Missing expected columns in df_base: {missing_base_columns}\"\n\ndf_technical = df.drop(columns=['reddit_sentiment', 'guardian_sentiment', 'reddit_score', 'nyt_sentiment'])\n# SANITY CHECK: Ensure subsets contain the expected columns\nexpected_technical_columns = set(df.columns) - {'reddit_sentiment', 'guardian_sentiment', 'reddit_score', 'nyt_sentiment'}\nmissing_technical_columns = expected_technical_columns - set(df_technical.columns)\nassert not missing_technical_columns, f\"Error: Missing expected columns in df_technical: {missing_technical_columns}\"\n\ndef xgboost_regressor(df, dataset_name):\n    # Preprocess data\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n    \n    # SANITY CHECK: Ensure 'Date' column is successfully converted to datetime\n    assert pd.api.types.is_datetime64_any_dtype(df.index), \"Error: 'Date' column is not properly converted to datetime.\"\n\n    # Create target variable (next day's High)\n    df['Target'] = df['High'].shift(-1)\n    df.dropna(subset=['Target'], inplace=True)\n\n    # SANITY CHECK: Ensure 'Target' column exists and has no missing values\n    assert 'Target' in df.columns, \"Error: 'Target' column not found in the DataFrame.\"\n    assert df['Target'].isnull().sum() == 0, \"Error: Missing values found in 'Target' column.\"\n    \n    # Features and target\n    features = df.drop(columns=['High', 'Target'])\n    target = df['Target']\n\n    # Split into train and test (last 30 days + 1 day prediction)\n    split_date = df.index[-31]\n    X_train, X_test = features[:split_date], features[split_date:]\n    y_train, y_test = target[:split_date], target[split_date:]\n\n    # Train mean prediction model\n    model_mean = xgb.XGBRegressor(\n        n_estimators=200,\n        learning_rate=0.1,\n        max_depth=5,\n        subsample=0.8,\n        colsample_bytree=0.7,\n        reg_lambda=5.0,  # Increase L2 regularization\n        reg_alpha=2.0,  # Add L1 regularization\n        gamma=0.5,\n        random_state=42\n    )\n    model_mean.fit(X_train, y_train)\n\n    # Train quantile models for 95% CI\n    base_params = model_mean.get_params()\n    base_params.pop('objective', None)  # Remove default objective\n\n    model_lower = xgb.XGBRegressor(\n        objective='reg:quantileerror',\n        quantile_alpha=0.1,  # Correct parameter name\n        **base_params\n    )\n    model_upper = xgb.XGBRegressor(\n        objective='reg:quantileerror',\n        quantile_alpha=0.90,  # Correct parameter name\n        **base_params\n    )\n\n    model_lower.fit(X_train, y_train)\n    model_upper.fit(X_train, y_train)\n\n    # Predictions\n    y_pred_mean = model_mean.predict(X_test)\n    y_pred_lower = model_lower.predict(X_test)\n    y_pred_upper = model_upper.predict(X_test)\n    \n    # SANITY CHECK: Ensure predictions have the same length as test data\n    assert len(y_pred_mean) == len(y_test), \"Error: Length mismatch between predicted and actual values.\"\n    assert len(y_pred_lower) == len(y_test), \"Error: Length mismatch between lower bound predictions and actual values.\"\n    assert len(y_pred_upper) == len(y_test), \"Error: Length mismatch between upper bound predictions and actual values.\"\n\n    # Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(y_test[:-1], y_pred_mean[:-1]))\n\n    # Feature importance\n    importance = pd.DataFrame({\n        'Feature': features.columns,\n        'Importance': model_mean.feature_importances_\n    }).sort_values(by='Importance', ascending=False)\n    \n    # SANITY CHECK: Ensure feature importance DataFrame is not empty\n    assert not importance.empty, \"Error: Feature importance DataFrame is empty.\"\n\n    # Plot results using Plotly\n    fig1 = go.Figure()\n\n    # Actual Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=y_test[:-1],\n        mode='lines+markers',\n        name='Actual Price',\n        marker=dict(color='blue')\n    ))\n\n    # Predicted Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=y_pred_mean[:-1],\n        mode='lines+markers',\n        name='Predicted Price',\n        line=dict(dash='dash'),\n        marker=dict(color='orange')\n    ))\n\n    # Confidence Interval\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1].tolist() + y_test.index[:-1].tolist()[::-1],\n        y=np.concatenate([y_pred_lower[:-1], y_pred_upper[:-1][::-1]]),\n        fill='toself',\n        fillcolor='rgba(128, 128, 128, 0.2)',\n        line=dict(color='rgba(255,255,255,0)'),\n        name='90% CI'\n    ))\n\n    # Next Day Prediction\n    last_date = y_test.index[-1] + pd.Timedelta(days=1)\n    fig1.add_trace(go.Scatter(\n        x=[last_date],\n        y=[y_pred_mean[-1]],\n        mode='markers',\n        name=f'Next Day Prediction ({y_pred_mean[-1]:.2f})',\n        marker=dict(color='red', size=10)\n    ))\n\n    fig1.update_layout(\n        title=f'Tesla Stock High Price Prediction with XGBoost&lt;br&gt;{dataset_name}',\n        xaxis_title='Date',\n        yaxis_title='Price ($)',\n        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.01),\n        template='plotly_white'\n    )\n    fig1.show()\n\n    # Feature Importance Plot\n    top_10_features = importance.head(10).sort_values(by='Importance', ascending=True)\n\n    # ‚û°Ô∏è Optional: Create a gradient color scale (you can remove if you want plain color)\n    colors = ['#1f77b4', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', '#ff7f0e']\n\n    fig2 = go.Figure()\n\n    fig2.add_trace(go.Bar(\n        y=top_10_features['Feature'],\n        x=top_10_features['Importance'],\n        orientation='h',\n        marker=dict(\n            color=\"lightblue\",\n            line=dict(color='black', width=1)\n        ),\n        name='Feature Importance',\n        hovertemplate='&lt;b&gt;%{y}&lt;/b&gt;: %{x:.4f}'  # Displays the score on hover\n    ))\n\n    # ‚û°Ô∏è Updated Layout Configuration for Figure 2\n    fig2.update_layout(\n        title={\n            'text': f'Top 10 Most Important Features&lt;br&gt;{dataset_name}',\n            'y': 0.95,                 # Pushed higher to avoid overlap\n            'x': 0.5,                  # Centered horizontally\n            'xanchor': 'center',\n            'yanchor': 'top',\n            'font': {'size': 18}       # Slightly larger for clarity\n        },\n        xaxis_title='Feature Importance Score',\n        yaxis_title='Features',\n        yaxis=dict(\n            autorange=False,\n            tickfont=dict(size=12),\n            categoryorder='total ascending'  # Ensures the most important is on top\n        ),\n        margin=dict(l=100, r=40, t=60, b=40),  # Extra space for better readability\n        template='plotly_white',\n        height=500  # Slightly taller for better clarity\n    )\n\n    fig2.show()\n\n    return {\n        'rmse': rmse,\n        'feature_importance': importance,\n        'next_day_prediction': y_pred_mean[-1]\n    }\n\n#SEGMENT: Analyzing Data with Only the Base Features : Open, High, Low, Close, Volume\nprint(\"Analyzing Data with Only the Base Features : Open, High, Low, Close, Volume\\n\")\nbase = xgboost_regressor(df_base, \"Base Data\")\nprint(base)\n\n\nAnalyzing Data with Only the Base Features : Open, High, Low, Close, Volume\n\n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n{'rmse': np.float64(12.460339799782533), 'feature_importance':   Feature  Importance\n1   Close    0.635840\n0     Low    0.195846\n3    Open    0.166153\n2  Volume    0.002160, 'next_day_prediction': np.float32(292.05157)}"
  },
  {
    "objectID": "results/technical-indicators.html#tesla-stock-high-price-prediction-with-random-forest-base-data-technical-indicators",
    "href": "results/technical-indicators.html#tesla-stock-high-price-prediction-with-random-forest-base-data-technical-indicators",
    "title": "Results & Insights",
    "section": "",
    "text": "Code\nimport warnings\nimport logging\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\ndf = pd.read_csv('final_data.csv')\n\n# SANITY CHECK: Ensure the DataFrame is not empty\nassert not df.empty, \"Error: The loaded DataFrame is empty.\"\n\ndf = df.fillna(0)\ndf[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].shift(1)\n\ndf_base = df.filter(items=['Date', 'High', 'Low', 'Close', 'Volume', \"Open\"])\ndf_technical = df.drop(columns=['reddit_sentiment', 'guardian_sentiment', 'reddit_score', 'nyt_sentiment'])\n\n# SANITY CHECK: Ensure subsets contain the expected columns\nexpected_base_columns = {'Date', 'High', 'Low', 'Close', 'Volume', 'Open'}\nmissing_base_columns = expected_base_columns - set(df_base.columns)\nassert not missing_base_columns, f\"Error: Missing expected columns in df_base: {missing_base_columns}\"\n\n\ndef random_forest_regression(df, dataset_name):\n    # Step 1: Data Preprocessing\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n    \n    # SANITY CHECK: Ensure 'Date' column is successfully converted to datetime\n    assert pd.api.types.is_datetime64_any_dtype(df.index), \"Error: 'Date' column is not properly converted to datetime.\"\n\n    # Create target variable (next day's High)\n    df['Target'] = df['High'].shift(-1)  # Predict next day's High\n\n    # Feature engineering\n    features = df.drop(columns=['High', 'Target'])\n    target = df['Target']\n\n    # Handle missing values after shifting\n    df.dropna(subset=['Target'], inplace=True)\n\n    # Split into train and test (last 30 days + 1 day prediction)\n    split_date = df.index[-31]\n    X_train, X_test = features[:split_date], features[split_date:]\n    y_train, y_test = target[:split_date], target[split_date:]\n\n    # Ensure X_train and X_test are pandas DataFrames with feature names\n    X_train = pd.DataFrame(X_train, columns=features.columns)\n    X_test = pd.DataFrame(X_test, columns=features.columns)\n\n    # Step 2: Train Random Forest Model\n    model_rf = RandomForestRegressor(\n        n_estimators=200,\n        max_depth=15,\n        min_samples_split=10,\n        min_samples_leaf=5,\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model_rf.fit(X_train, y_train)\n\n    # Step 3: Make Predictions\n    y_pred = model_rf.predict(X_test)\n\n    # Step 4: Estimate Confidence Intervals using Prediction Variance\n    # Get predictions from each tree in the forest\n    predictions_per_tree = np.array([tree.predict(X_test) for tree in model_rf.estimators_])\n\n    # Calculate mean and percentiles across trees\n    mean_predictions = predictions_per_tree.mean(axis=0)\n    lower_bound = np.percentile(predictions_per_tree, 5, axis=0)  # 5th percentile for 90% CI\n    upper_bound = np.percentile(predictions_per_tree, 95, axis=0)  # 95th percentile for 90% CI\n    \n    # SANITY CHECK: Ensure predictions have the same length as test data\n    assert len(mean_predictions) == len(y_test), \"Error: Length mismatch between predicted and actual values.\"\n    assert len(lower_bound) == len(y_test), \"Error: Length mismatch between lower bound predictions and actual values.\"\n    assert len(upper_bound) == len(y_test), \"Error: Length mismatch between upper bound predictions and actual values.\"\n\n    # Step 5: Feature Importance\n    feature_importance = pd.DataFrame({\n        'Feature': X_train.columns,\n        'Importance': model_rf.feature_importances_\n    }).sort_values(by='Importance', ascending=False)\n    \n    # SANITY CHECK: Ensure feature importance DataFrame is not empty\n    assert not feature_importance.empty, \"Error: Feature importance DataFrame is empty.\"\n\n\n    # Step 6: Plot Results using Plotly\n    fig1 = go.Figure()\n\n    # Actual Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=y_test[:-1],\n        mode='lines+markers',\n        name='Actual Price',\n        marker=dict(color='blue')\n    ))\n\n    # Predicted Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=mean_predictions[:-1],\n        mode='lines+markers',\n        name='Predicted Price',\n        line=dict(dash='dash'),\n        marker=dict(color='orange')\n    ))\n\n    # Confidence Interval\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1].tolist() + y_test.index[:-1].tolist()[::-1],\n        y=np.concatenate([lower_bound[:-1], upper_bound[:-1][::-1]]),\n        fill='toself',\n        fillcolor='rgba(128, 128, 128, 0.2)',\n        line=dict(color='rgba(255,255,255,0)'),\n        name='90% CI'\n    ))\n\n    # Next Day Prediction\n    last_date = y_test.index[-1] + pd.Timedelta(days=1)\n    fig1.add_trace(go.Scatter(\n        x=[last_date],\n        y=[mean_predictions[-1]],\n        mode='markers',\n        name=f'Next Day Prediction ({mean_predictions[-1]:.2f})',\n        marker=dict(color='red', size=10)\n    ))\n\n    fig1.update_layout(\n        title=f'Tesla Stock High Price Prediction with Random Forest&lt;br&gt;{dataset_name}',\n        xaxis_title='Date',\n        yaxis_title='Price ($)',\n        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.01),\n        template='plotly_white'\n    )\n    fig1.show()\n\n    # Feature Importance Plot\n    top_10_features = feature_importance.head(10)\n    fig2 = go.Figure()\n\n    fig2.add_trace(go.Bar(\n        y=top_10_features['Feature'],\n        x=top_10_features['Importance'],\n        orientation='h',\n        marker=dict(color='skyblue', line=dict(color='black', width=1)),\n        name='Feature Importance'\n    ))\n\n    # ‚û°Ô∏è Updated Layout Configuration for Figure 2\n    fig2.update_layout(\n        title={\n            'text': f'Top 10 Most Important Features&lt;br&gt;{dataset_name}',\n            'y': 0.95,                 # Pushed higher to avoid overlap\n            'x': 0.5,                  # Centered horizontally\n            'xanchor': 'center',\n            'yanchor': 'top',\n            'font': {'size': 18}       # Slightly larger for clarity\n        },\n        xaxis_title='Feature Importance Score',\n        yaxis_title='Features',\n        yaxis=dict(autorange=\"reversed\"),\n        margin=dict(l=60, r=40, t=60, b=40),  # Extra space to avoid crowding\n        template='plotly_white',\n        height=500  # Slightly taller for better readability\n    )\n\n    fig2.show()\n\n    # Step 7: Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(y_test[:-1], mean_predictions[:-1]))\n\n    return {\n        'rmse': rmse,\n        'next_day_prediction': mean_predictions[-1]\n    }\n\n\n\n\n\n#SEGMENT: Analyzing Data with Base Features + Technical Indicators : RSI, MACD, etc\nprint(\"Analyzing Data with Base Features + Technical Indicators : RSI, MACD, etc\\n\")\ntechnical = random_forest_regression(df_technical, \"Base Data + Technical Indicators\")\nprint(technical)\n\n\nAnalyzing Data with Base Features + Technical Indicators : RSI, MACD, etc\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n{'rmse': np.float64(9.756950435689252), 'next_day_prediction': np.float64(278.78239886219393)}"
  },
  {
    "objectID": "results/technical-indicators.html#tesla-stock-high-price-prediction-with-xgboost-base-data-technical-indicators",
    "href": "results/technical-indicators.html#tesla-stock-high-price-prediction-with-xgboost-base-data-technical-indicators",
    "title": "Results & Insights",
    "section": "Tesla Stock High Price Prediction With XGBoost (Base Data + Technical Indicators)",
    "text": "Tesla Stock High Price Prediction With XGBoost (Base Data + Technical Indicators)\n\n\nCode\nimport warnings\nimport logging\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nimport plotly.graph_objects as go\n\ndf = pd.read_csv('final_data.csv')\n\n# SANITY CHECK: Ensure the DataFrame is not empty\nassert not df.empty, \"Error: The loaded DataFrame is empty.\"\n\ndf[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].shift(1)\ndf = df.fillna(0)\n# SANITY CHECK: Verify no missing values after shifting\nassert df[['Open', 'High', 'Low', 'Close', 'Volume']].isnull().sum().sum() == 0, \"Error: Missing values found after shifting.\"\n\n\ndf_base = df.filter(items=['Date', 'High', 'Low', 'Close', 'Volume', \"Open\"])\n# SANITY CHECK: Ensure subsets contain the expected columns\nexpected_base_columns = {'Date', 'High', 'Low', 'Close', 'Volume', 'Open'}\nmissing_base_columns = expected_base_columns - set(df_base.columns)\nassert not missing_base_columns, f\"Error: Missing expected columns in df_base: {missing_base_columns}\"\n\ndf_technical = df.drop(columns=['reddit_sentiment', 'guardian_sentiment', 'reddit_score', 'nyt_sentiment'])\n# SANITY CHECK: Ensure subsets contain the expected columns\nexpected_technical_columns = set(df.columns) - {'reddit_sentiment', 'guardian_sentiment', 'reddit_score', 'nyt_sentiment'}\nmissing_technical_columns = expected_technical_columns - set(df_technical.columns)\nassert not missing_technical_columns, f\"Error: Missing expected columns in df_technical: {missing_technical_columns}\"\n\ndef xgboost_regressor(df, dataset_name):\n    # Preprocess data\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n    \n    # SANITY CHECK: Ensure 'Date' column is successfully converted to datetime\n    assert pd.api.types.is_datetime64_any_dtype(df.index), \"Error: 'Date' column is not properly converted to datetime.\"\n\n    # Create target variable (next day's High)\n    df['Target'] = df['High'].shift(-1)\n    df.dropna(subset=['Target'], inplace=True)\n\n    # SANITY CHECK: Ensure 'Target' column exists and has no missing values\n    assert 'Target' in df.columns, \"Error: 'Target' column not found in the DataFrame.\"\n    assert df['Target'].isnull().sum() == 0, \"Error: Missing values found in 'Target' column.\"\n    \n    # Features and target\n    features = df.drop(columns=['High', 'Target'])\n    target = df['Target']\n\n    # Split into train and test (last 30 days + 1 day prediction)\n    split_date = df.index[-31]\n    X_train, X_test = features[:split_date], features[split_date:]\n    y_train, y_test = target[:split_date], target[split_date:]\n\n    # Train mean prediction model\n    model_mean = xgb.XGBRegressor(\n        n_estimators=200,\n        learning_rate=0.1,\n        max_depth=5,\n        subsample=0.8,\n        colsample_bytree=0.7,\n        reg_lambda=5.0,  # Increase L2 regularization\n        reg_alpha=2.0,  # Add L1 regularization\n        gamma=0.5,\n        random_state=42\n    )\n    model_mean.fit(X_train, y_train)\n\n    # Train quantile models for 95% CI\n    base_params = model_mean.get_params()\n    base_params.pop('objective', None)  # Remove default objective\n\n    model_lower = xgb.XGBRegressor(\n        objective='reg:quantileerror',\n        quantile_alpha=0.1,  # Correct parameter name\n        **base_params\n    )\n    model_upper = xgb.XGBRegressor(\n        objective='reg:quantileerror',\n        quantile_alpha=0.90,  # Correct parameter name\n        **base_params\n    )\n\n    model_lower.fit(X_train, y_train)\n    model_upper.fit(X_train, y_train)\n\n    # Predictions\n    y_pred_mean = model_mean.predict(X_test)\n    y_pred_lower = model_lower.predict(X_test)\n    y_pred_upper = model_upper.predict(X_test)\n    \n    # SANITY CHECK: Ensure predictions have the same length as test data\n    assert len(y_pred_mean) == len(y_test), \"Error: Length mismatch between predicted and actual values.\"\n    assert len(y_pred_lower) == len(y_test), \"Error: Length mismatch between lower bound predictions and actual values.\"\n    assert len(y_pred_upper) == len(y_test), \"Error: Length mismatch between upper bound predictions and actual values.\"\n\n    # Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(y_test[:-1], y_pred_mean[:-1]))\n\n    # Feature importance\n    importance = pd.DataFrame({\n        'Feature': features.columns,\n        'Importance': model_mean.feature_importances_\n    }).sort_values(by='Importance', ascending=False)\n    \n    # SANITY CHECK: Ensure feature importance DataFrame is not empty\n    assert not importance.empty, \"Error: Feature importance DataFrame is empty.\"\n\n    # Plot results using Plotly\n    fig1 = go.Figure()\n\n    # Actual Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=y_test[:-1],\n        mode='lines+markers',\n        name='Actual Price',\n        marker=dict(color='blue')\n    ))\n\n    # Predicted Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=y_pred_mean[:-1],\n        mode='lines+markers',\n        name='Predicted Price',\n        line=dict(dash='dash'),\n        marker=dict(color='orange')\n    ))\n\n    # Confidence Interval\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1].tolist() + y_test.index[:-1].tolist()[::-1],\n        y=np.concatenate([y_pred_lower[:-1], y_pred_upper[:-1][::-1]]),\n        fill='toself',\n        fillcolor='rgba(128, 128, 128, 0.2)',\n        line=dict(color='rgba(255,255,255,0)'),\n        name='90% CI'\n    ))\n\n    # Next Day Prediction\n    last_date = y_test.index[-1] + pd.Timedelta(days=1)\n    fig1.add_trace(go.Scatter(\n        x=[last_date],\n        y=[y_pred_mean[-1]],\n        mode='markers',\n        name=f'Next Day Prediction ({y_pred_mean[-1]:.2f})',\n        marker=dict(color='red', size=10)\n    ))\n\n    fig1.update_layout(\n        title=f'Tesla Stock High Price Prediction with XGBoost&lt;br&gt;{dataset_name}',\n        xaxis_title='Date',\n        yaxis_title='Price ($)',\n        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.01),\n        template='plotly_white'\n    )\n    fig1.show()\n\n    # Feature Importance Plot\n    top_10_features = importance.head(10)\n    fig2 = go.Figure()\n\n    fig2.add_trace(go.Bar(\n        y=top_10_features['Feature'],\n        x=top_10_features['Importance'],\n        orientation='h',\n        marker=dict(\n            color='skyblue',\n            line=dict(color='black', width=1)\n        ),\n        name='Feature Importance'\n    ))\n\n    # ‚û°Ô∏è Updated Layout Configuration for Figure 2\n    fig2.update_layout(\n        title={\n            'text': f'Top 10 Most Important Features&lt;br&gt;{dataset_name}',\n            'y': 0.95,                 # Pushed higher to avoid overlap\n            'x': 0.5,                  # Centered horizontally\n            'xanchor': 'center',\n            'yanchor': 'top',\n            'font': {'size': 18}       # Slightly larger for clarity\n        },\n        xaxis_title='Feature Importance Score',\n        yaxis_title='Features',\n        yaxis=dict(\n            autorange=\"reversed\",  # Keeps the most important feature on top\n            tickfont=dict(size=12) # Increases readability of feature names\n        ),\n        margin=dict(l=100, r=40, t=60, b=40),  # Extra space to avoid crowding\n        template='plotly_white',\n        height=500  # Slightly taller for better readability\n    )\n\n    fig2.show()\n    return {\n        'rmse': rmse,\n        'next_day_prediction': y_pred_mean[-1]\n    }\n\n\n\n#SEGMENT: Analyzing Data with Base Features + Technical Indicators : RSI, MACD, etc\nprint(\"Analyzing Data with Base Features + Technical Indicators : RSI, MACD, etc\\n\")\ntechnical = xgboost_regressor(df_technical, \"Base Data + Technical Indicators\")\nprint(technical)\n\n\nAnalyzing Data with Base Features + Technical Indicators : RSI, MACD, etc\n\n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n{'rmse': np.float64(7.362339808947036), 'next_day_prediction': np.float32(282.21558)}"
  },
  {
    "objectID": "results/sentiment-analysis.html#tesla-stock-high-price-prediction-with-random-forest-base-data-technical-indicators-sentiment",
    "href": "results/sentiment-analysis.html#tesla-stock-high-price-prediction-with-random-forest-base-data-technical-indicators-sentiment",
    "title": "Results & Insights",
    "section": "",
    "text": "Code\nimport warnings\nimport logging\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\ndf = pd.read_csv('final_data.csv')\n\n# SANITY CHECK: Ensure the DataFrame is not empty\nassert not df.empty, \"Error: The loaded DataFrame is empty.\"\n\ndf = df.fillna(0)\ndf[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].shift(1)\n\ndf_base = df.filter(items=['Date', 'High', 'Low', 'Close', 'Volume', \"Open\"])\ndf_technical = df.drop(columns=['reddit_sentiment', 'guardian_sentiment', 'reddit_score', 'nyt_sentiment'])\n\n# SANITY CHECK: Ensure subsets contain the expected columns\nexpected_base_columns = {'Date', 'High', 'Low', 'Close', 'Volume', 'Open'}\nmissing_base_columns = expected_base_columns - set(df_base.columns)\nassert not missing_base_columns, f\"Error: Missing expected columns in df_base: {missing_base_columns}\"\n\n\ndef random_forest_regression(df, dataset_name):\n    # Step 1: Data Preprocessing\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n    \n    # SANITY CHECK: Ensure 'Date' column is successfully converted to datetime\n    assert pd.api.types.is_datetime64_any_dtype(df.index), \"Error: 'Date' column is not properly converted to datetime.\"\n\n    # Create target variable (next day's High)\n    df['Target'] = df['High'].shift(-1)  # Predict next day's High\n\n    # Feature engineering\n    features = df.drop(columns=['High', 'Target'])\n    target = df['Target']\n\n    # Handle missing values after shifting\n    df.dropna(subset=['Target'], inplace=True)\n\n    # Split into train and test (last 30 days + 1 day prediction)\n    split_date = df.index[-31]\n    X_train, X_test = features[:split_date], features[split_date:]\n    y_train, y_test = target[:split_date], target[split_date:]\n\n    # Ensure X_train and X_test are pandas DataFrames with feature names\n    X_train = pd.DataFrame(X_train, columns=features.columns)\n    X_test = pd.DataFrame(X_test, columns=features.columns)\n\n    # Step 2: Train Random Forest Model\n    model_rf = RandomForestRegressor(\n        n_estimators=200,\n        max_depth=15,\n        min_samples_split=10,\n        min_samples_leaf=5,\n        random_state=42,\n        n_jobs=-1\n    )\n\n    model_rf.fit(X_train, y_train)\n\n    # Step 3: Make Predictions\n    y_pred = model_rf.predict(X_test)\n\n    # Step 4: Estimate Confidence Intervals using Prediction Variance\n    # Get predictions from each tree in the forest\n    predictions_per_tree = np.array([tree.predict(X_test) for tree in model_rf.estimators_])\n\n    # Calculate mean and percentiles across trees\n    mean_predictions = predictions_per_tree.mean(axis=0)\n    lower_bound = np.percentile(predictions_per_tree, 5, axis=0)  # 5th percentile for 90% CI\n    upper_bound = np.percentile(predictions_per_tree, 95, axis=0)  # 95th percentile for 90% CI\n    \n    # SANITY CHECK: Ensure predictions have the same length as test data\n    assert len(mean_predictions) == len(y_test), \"Error: Length mismatch between predicted and actual values.\"\n    assert len(lower_bound) == len(y_test), \"Error: Length mismatch between lower bound predictions and actual values.\"\n    assert len(upper_bound) == len(y_test), \"Error: Length mismatch between upper bound predictions and actual values.\"\n\n    # Step 5: Feature Importance\n    feature_importance = pd.DataFrame({\n        'Feature': X_train.columns,\n        'Importance': model_rf.feature_importances_\n    }).sort_values(by='Importance', ascending=False)\n    \n    # SANITY CHECK: Ensure feature importance DataFrame is not empty\n    assert not feature_importance.empty, \"Error: Feature importance DataFrame is empty.\"\n\n\n    # Step 6: Plot Results using Plotly\n    fig1 = go.Figure()\n\n    # Actual Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=y_test[:-1],\n        mode='lines+markers',\n        name='Actual Price',\n        marker=dict(color='blue')\n    ))\n\n    # Predicted Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=mean_predictions[:-1],\n        mode='lines+markers',\n        name='Predicted Price',\n        line=dict(dash='dash'),\n        marker=dict(color='orange')\n    ))\n\n    # Confidence Interval\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1].tolist() + y_test.index[:-1].tolist()[::-1],\n        y=np.concatenate([lower_bound[:-1], upper_bound[:-1][::-1]]),\n        fill='toself',\n        fillcolor='rgba(128, 128, 128, 0.2)',\n        line=dict(color='rgba(255,255,255,0)'),\n        name='90% CI'\n    ))\n\n    # Next Day Prediction\n    last_date = y_test.index[-1] + pd.Timedelta(days=1)\n    fig1.add_trace(go.Scatter(\n        x=[last_date],\n        y=[mean_predictions[-1]],\n        mode='markers',\n        name=f'Next Day Prediction ({mean_predictions[-1]:.2f})',\n        marker=dict(color='red', size=10)\n    ))\n\n    fig1.update_layout(\n        title=f'Tesla Stock High Price Prediction with Random Forest&lt;br&gt;{dataset_name}',\n        xaxis_title='Date',\n        yaxis_title='Price ($)',\n        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.01),\n        template='plotly_white'\n    )\n    fig1.show()\n\n    # Feature Importance Plot\n    top_10_features = feature_importance.head(10)\n    fig2 = go.Figure()\n\n    # ‚û°Ô∏è Optional: Create a gradient color scale (you can remove if you want plain color)\n    colors = \"lightblue\"\n\n    fig2.add_trace(go.Bar(\n        y=top_10_features['Feature'],\n        x=top_10_features['Importance'],\n        orientation='h',\n        marker=dict(\n            color=colors,\n            line=dict(color='black', width=1)\n        ),\n        name='Feature Importance'\n    ))\n\n    # ‚û°Ô∏è Updated Layout Configuration for Figure 2\n    fig2.update_layout(\n        title={\n            'text': f'Top 10 Most Important Features&lt;br&gt;{dataset_name}',\n            'y': 0.95,                 # Pushed higher to avoid overlap\n            'x': 0.5,                  # Centered horizontally\n            'xanchor': 'center',\n            'yanchor': 'top',\n            'font': {'size': 18}       # Slightly larger for clarity\n        },\n        xaxis_title='Feature Importance Score',\n        yaxis_title='Features',\n        yaxis=dict(\n            autorange=\"reversed\",     # Most important features on top\n            tickfont=dict(size=12)    # Larger font for better readability\n        ),\n        margin=dict(l=100, r=40, t=60, b=40),  # Extra space for better readability\n        template='plotly_white',\n        height=500  # Slightly taller for better clarity\n    )\n\n    fig2.show()\n\n    # Step 7: Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(y_test[:-1], mean_predictions[:-1]))\n\n    return {\n        'rmse': rmse,\n        'next_day_prediction': mean_predictions[-1]\n    }\n\n\n\n\n\n#SEGMENT: Analyzing Data with Base Features + Technical Indicators + Sentiment Analysis: Reddit, Guardian, NYT\nprint(\"Analyzing Data with Base Features + Technical Indicators + Sentiment Analysis: Reddit, Guardian, NYT\\n\")\nsentiment = random_forest_regression(df, \"Base Data + Technical Indicators + Sentiment Analysis\")\nprint(sentiment)\n\n\nAnalyzing Data with Base Features + Technical Indicators + Sentiment Analysis: Reddit, Guardian, NYT\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n{'rmse': np.float64(9.771690065263131), 'next_day_prediction': np.float64(279.24225001408746)}"
  },
  {
    "objectID": "results/sentiment-analysis.html#tesla-stock-high-price-prediction-with-xgboost-base-data-technical-indicators-sentiment",
    "href": "results/sentiment-analysis.html#tesla-stock-high-price-prediction-with-xgboost-base-data-technical-indicators-sentiment",
    "title": "Results & Insights",
    "section": "Tesla Stock High Price Prediction With XGBoost (Base Data + Technical Indicators + Sentiment)",
    "text": "Tesla Stock High Price Prediction With XGBoost (Base Data + Technical Indicators + Sentiment)\n\n\nCode\nimport warnings\nimport logging\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nimport plotly.graph_objects as go\n\ndf = pd.read_csv('final_data.csv')\n\n# SANITY CHECK: Ensure the DataFrame is not empty\nassert not df.empty, \"Error: The loaded DataFrame is empty.\"\n\ndf[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].shift(1)\ndf = df.fillna(0)\n# SANITY CHECK: Verify no missing values after shifting\nassert df[['Open', 'High', 'Low', 'Close', 'Volume']].isnull().sum().sum() == 0, \"Error: Missing values found after shifting.\"\n\n\ndf_base = df.filter(items=['Date', 'High', 'Low', 'Close', 'Volume', \"Open\"])\n# SANITY CHECK: Ensure subsets contain the expected columns\nexpected_base_columns = {'Date', 'High', 'Low', 'Close', 'Volume', 'Open'}\nmissing_base_columns = expected_base_columns - set(df_base.columns)\nassert not missing_base_columns, f\"Error: Missing expected columns in df_base: {missing_base_columns}\"\n\ndf_technical = df.drop(columns=['reddit_sentiment', 'guardian_sentiment', 'reddit_score', 'nyt_sentiment'])\n# SANITY CHECK: Ensure subsets contain the expected columns\nexpected_technical_columns = set(df.columns) - {'reddit_sentiment', 'guardian_sentiment', 'reddit_score', 'nyt_sentiment'}\nmissing_technical_columns = expected_technical_columns - set(df_technical.columns)\nassert not missing_technical_columns, f\"Error: Missing expected columns in df_technical: {missing_technical_columns}\"\n\ndef xgboost_regressor(df, dataset_name):\n    # Preprocess data\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n    \n    # SANITY CHECK: Ensure 'Date' column is successfully converted to datetime\n    assert pd.api.types.is_datetime64_any_dtype(df.index), \"Error: 'Date' column is not properly converted to datetime.\"\n\n    # Create target variable (next day's High)\n    df['Target'] = df['High'].shift(-1)\n    df.dropna(subset=['Target'], inplace=True)\n\n    # SANITY CHECK: Ensure 'Target' column exists and has no missing values\n    assert 'Target' in df.columns, \"Error: 'Target' column not found in the DataFrame.\"\n    assert df['Target'].isnull().sum() == 0, \"Error: Missing values found in 'Target' column.\"\n    \n    # Features and target\n    features = df.drop(columns=['High', 'Target'])\n    target = df['Target']\n\n    # Split into train and test (last 30 days + 1 day prediction)\n    split_date = df.index[-31]\n    X_train, X_test = features[:split_date], features[split_date:]\n    y_train, y_test = target[:split_date], target[split_date:]\n\n    # Train mean prediction model\n    model_mean = xgb.XGBRegressor(\n        n_estimators=200,\n        learning_rate=0.1,\n        max_depth=5,\n        subsample=0.8,\n        colsample_bytree=0.7,\n        reg_lambda=5.0,  # Increase L2 regularization\n        reg_alpha=2.0,  # Add L1 regularization\n        gamma=0.5,\n        random_state=42\n    )\n    model_mean.fit(X_train, y_train)\n\n    # Train quantile models for 95% CI\n    base_params = model_mean.get_params()\n    base_params.pop('objective', None)  # Remove default objective\n\n    model_lower = xgb.XGBRegressor(\n        objective='reg:quantileerror',\n        quantile_alpha=0.1,  # Correct parameter name\n        **base_params\n    )\n    model_upper = xgb.XGBRegressor(\n        objective='reg:quantileerror',\n        quantile_alpha=0.90,  # Correct parameter name\n        **base_params\n    )\n\n    model_lower.fit(X_train, y_train)\n    model_upper.fit(X_train, y_train)\n\n    # Predictions\n    y_pred_mean = model_mean.predict(X_test)\n    y_pred_lower = model_lower.predict(X_test)\n    y_pred_upper = model_upper.predict(X_test)\n    \n    # SANITY CHECK: Ensure predictions have the same length as test data\n    assert len(y_pred_mean) == len(y_test), \"Error: Length mismatch between predicted and actual values.\"\n    assert len(y_pred_lower) == len(y_test), \"Error: Length mismatch between lower bound predictions and actual values.\"\n    assert len(y_pred_upper) == len(y_test), \"Error: Length mismatch between upper bound predictions and actual values.\"\n\n    # Calculate RMSE\n    rmse = np.sqrt(mean_squared_error(y_test[:-1], y_pred_mean[:-1]))\n\n    # Feature importance\n    importance = pd.DataFrame({\n        'Feature': features.columns,\n        'Importance': model_mean.feature_importances_\n    }).sort_values(by='Importance', ascending=False)\n    \n    # SANITY CHECK: Ensure feature importance DataFrame is not empty\n    assert not importance.empty, \"Error: Feature importance DataFrame is empty.\"\n\n    # Plot results using Plotly\n    fig1 = go.Figure()\n\n    # Actual Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=y_test[:-1],\n        mode='lines+markers',\n        name='Actual Price',\n        marker=dict(color='blue')\n    ))\n\n    # Predicted Price\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1],\n        y=y_pred_mean[:-1],\n        mode='lines+markers',\n        name='Predicted Price',\n        line=dict(dash='dash'),\n        marker=dict(color='orange')\n    ))\n\n    # Confidence Interval\n    fig1.add_trace(go.Scatter(\n        x=y_test.index[:-1].tolist() + y_test.index[:-1].tolist()[::-1],\n        y=np.concatenate([y_pred_lower[:-1], y_pred_upper[:-1][::-1]]),\n        fill='toself',\n        fillcolor='rgba(128, 128, 128, 0.2)',\n        line=dict(color='rgba(255,255,255,0)'),\n        name='90% CI'\n    ))\n\n    # Next Day Prediction\n    last_date = y_test.index[-1] + pd.Timedelta(days=1)\n    fig1.add_trace(go.Scatter(\n        x=[last_date],\n        y=[y_pred_mean[-1]],\n        mode='markers',\n        name=f'Next Day Prediction ({y_pred_mean[-1]:.2f})',\n        marker=dict(color='red', size=10)\n    ))\n\n    fig1.update_layout(\n        title=f'Tesla Stock High Price Prediction with XGBoost&lt;br&gt;{dataset_name}',\n        xaxis_title='Date',\n        yaxis_title='Price ($)',\n        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.01),\n        template='plotly_white'\n    )\n    fig1.show()\n\n    # Feature Importance Plot\n    top_10_features = importance.head(10)\n    fig2 = go.Figure()\n\n    fig2.add_trace(go.Bar(\n        y=top_10_features['Feature'],\n        x=top_10_features['Importance'],\n        orientation='h',\n        marker=dict(color='skyblue', line=dict(color='black', width=1)),\n        name='Feature Importance'\n    ))\n\n    fig2.update_layout(\n        title=f'Top 10 Most Important Features&lt;br&gt;{dataset_name}',\n        xaxis_title='Feature Importance Score',\n        yaxis_title='Features',\n        yaxis=dict(autorange=\"reversed\"),\n        template='plotly_white'\n    )\n    fig2.show()\n\n    return {\n        'rmse': rmse,\n        'next_day_prediction': y_pred_mean[-1]\n    }\n\n\n#SEGMENT: Analyzing Data with Base Features + Technical Indicators + Sentiment Analysis: Reddit, Guardian, NYT\nprint(\"Analyzing Data with Base Features + Technical Indicators + Sentiment Analysis: Reddit, Guardian, NYT\\n\")\nsentiment = xgboost_regressor(df, \"Base Data + Technical Indicators + Sentiment Analysis\")\nprint(sentiment)\n\n\nAnalyzing Data with Base Features + Technical Indicators + Sentiment Analysis: Reddit, Guardian, NYT\n\n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n{'rmse': np.float64(7.756901095170007), 'next_day_prediction': np.float32(283.5838)}"
  },
  {
    "objectID": "exploratory-analysis.html#tesla-return-and-nyt-sentiment-march-2427-2025",
    "href": "exploratory-analysis.html#tesla-return-and-nyt-sentiment-march-2427-2025",
    "title": "Exploratory Data Analysis",
    "section": "üîç Tesla: Return %, and NYT Sentiment (March 24‚Äì27, 2025)",
    "text": "üîç Tesla: Return %, and NYT Sentiment (March 24‚Äì27, 2025)\nThis multi-panel chart combines Tesla‚Äôs daily candlestick prices with return percentages and New York Times sentiment scores.\n\nMarch 24 shows a strong upward reversal, with green candlesticks and rising returns aligning with a slightly negative NYT sentiment.\nA positive return on March 25 coincides with improving sentiment, while March 26 sees both sentiment and return drop sharply.\nBy March 27, despite neutral price movement, sentiment plunges‚Äîhighlighting a potential disconnect between news tone and market behavior.\nThis suggests sentiment shifts may lag or diverge from short-term price dynamics.\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom matplotlib.patches import Rectangle\n\ndf = pd.read_csv(\"final_data.csv\")\ndf.columns = df.columns.str.strip()\ndf['Date'] = pd.to_datetime(df['Date'])\n\nstart_date = pd.to_datetime(\"2025-03-24\")\nend_date = start_date + pd.Timedelta(days=3)\nmarch_df = df[(df['Date'] &gt;= start_date) & (df['Date'] &lt;= end_date)].copy()\nmarch_df['Daily_Return_%'] = march_df['Close'].pct_change() * 100\n\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10), sharex=True, gridspec_kw={'height_ratios': [3, 1, 1]})\n\n# --- Candlestick Plot ---\nfor _, row in march_df.iterrows():\n    date_num = mdates.date2num(row['Date'])\n    color = 'green' if row['Close'] &gt;= row['Open'] else 'red'\n\n    # Wick\n    ax1.plot([row['Date'], row['Date']], [row['Low'], row['High']], color='black')\n\n    # Body\n    rect = Rectangle((date_num - 0.2, min(row['Open'], row['Close'])),\n                     0.4,\n                     abs(row['Close'] - row['Open']),\n                     color=color)\n    ax1.add_patch(rect)\n\nax1.set_ylabel(\"Price ($)\")\nax1.set_title(\"Tesla: Return %, and NYT Sentiment (March 24‚Äì27, 2025)\")\nax1.grid(True)\n\n# --- Daily Return Plot ---\nax2.bar(march_df['Date'], march_df['Daily_Return_%'], color='skyblue')\nax2.axhline(0, color='black', linestyle='--')\nax2.set_ylabel(\"Return (%)\")\nax2.grid(True)\n\n# --- FinBERT Sentiment Plot ---\nax3.plot(march_df['Date'], march_df['nyt_sentiment'], marker='o', linestyle='--', color='red')\nax3.set_ylabel(\"NEW YORK TIME Sentiment\")\nax3.set_xlabel(\"Date\")\nax3.grid(True)\n\nax3.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\nfig.autofmt_xdate()\nplt.tight_layout()\nplt.show()"
  }
]